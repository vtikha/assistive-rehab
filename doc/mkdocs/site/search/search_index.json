{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Assistive-rehab \u00b6 Assistive-rehab is a framework for developing the assistive intelligence of R1 robot for clinical rehabilitation. The project is being developed within the Joint Lab between IIT and Fondazione Don Carlo Gnocchi Onlus . Scenario \u00b6 In a typical scenario, R1 guides a group of patients in performing physical exercises for the upper limbs and the torso. The robot physically shows to each patient the specific exercise to perform, while verifying the correct execution and providing a verbal feedback in real-time. Components \u00b6 The demo is implemented through the following components: R1 is equipped with an Intel RealSense D435 depth camera, which provides RGB images along with depth data; acquisition and storage are carried out by the ACQ and the OPC module respectively. The ACQ module merges 2D skeleton data, as acquired by yarpOpenPose , along with the depth information provided by the depth sensor. 3D skeletons are stored in the OPC. This architecture also allows to replay experiments , using the combination of yarpdatadumper and yarpdataplayer , giving to physiotherapists the possibility of computing new metrics (not run online) on the replayed experiment, and of maintaining a logbook of all the patient's exercises; the face recognition pipeline is responsible for associating a labeled face to a skeleton, which thus becomes unambiguously tagged. This allows potentially to personalize exercises according to the specific rehabilitation path of a patient; the analysis of motion extracts in real-time salient metrics, as the Range of Motion (RoM) of the articular angles (for abduction and internal/external rotation), and the speed of the end-point (for a reaching task). The module also exports relevant data for enabling offline reporting of the experiments at the end of the session. The motion analysis provides a useful tool to physiotherapists for both evaluating the quality of movement of a patient and benefiting from the offline report as required documentation to produce after a session; the final feedback is produced by means of an action recognizer and a feedback analysis. The action recognizer uses 2D keypoints trajectories to predict the label of the performed exercise and triggers the analysis only if the predicted label is the same as the exercise to perform. If so, the analysis compares the observed skeleton with a (predefined) template skeleton to produce real-time feedback on the range of motion, the speed and how well the target is reached; the robot's movement is controlled through ctpService ; the interactionManager manages the interaction between all the modules involved.","title":"Home"},{"location":"#assistive-rehab","text":"Assistive-rehab is a framework for developing the assistive intelligence of R1 robot for clinical rehabilitation. The project is being developed within the Joint Lab between IIT and Fondazione Don Carlo Gnocchi Onlus .","title":"Assistive-rehab"},{"location":"#scenario","text":"In a typical scenario, R1 guides a group of patients in performing physical exercises for the upper limbs and the torso. The robot physically shows to each patient the specific exercise to perform, while verifying the correct execution and providing a verbal feedback in real-time.","title":"Scenario"},{"location":"#components","text":"The demo is implemented through the following components: R1 is equipped with an Intel RealSense D435 depth camera, which provides RGB images along with depth data; acquisition and storage are carried out by the ACQ and the OPC module respectively. The ACQ module merges 2D skeleton data, as acquired by yarpOpenPose , along with the depth information provided by the depth sensor. 3D skeletons are stored in the OPC. This architecture also allows to replay experiments , using the combination of yarpdatadumper and yarpdataplayer , giving to physiotherapists the possibility of computing new metrics (not run online) on the replayed experiment, and of maintaining a logbook of all the patient's exercises; the face recognition pipeline is responsible for associating a labeled face to a skeleton, which thus becomes unambiguously tagged. This allows potentially to personalize exercises according to the specific rehabilitation path of a patient; the analysis of motion extracts in real-time salient metrics, as the Range of Motion (RoM) of the articular angles (for abduction and internal/external rotation), and the speed of the end-point (for a reaching task). The module also exports relevant data for enabling offline reporting of the experiments at the end of the session. The motion analysis provides a useful tool to physiotherapists for both evaluating the quality of movement of a patient and benefiting from the offline report as required documentation to produce after a session; the final feedback is produced by means of an action recognizer and a feedback analysis. The action recognizer uses 2D keypoints trajectories to predict the label of the performed exercise and triggers the analysis only if the predicted label is the same as the exercise to perform. If so, the analysis compares the observed skeleton with a (predefined) template skeleton to produce real-time feedback on the range of motion, the speed and how well the target is reached; the robot's movement is controlled through ctpService ; the interactionManager manages the interaction between all the modules involved.","title":"Components"},{"location":"Y1M5/","text":"Y1M5 \u00b6 This application has been developed during the first semester of 2018 with the aim of starting implementing the features required to support R1 as artificial assistant of the physiotherapist. The fundamental aspects considered are the following: R1 engages the user, proposing a rehabilitative exercise of the upper limbs, chosen from a predefined repertoire; the exercise the user has to perform is shown on the screen, through a template skeleton; R1 analyzes in real-time the execution of the exercise, providing a verbal feedback limited to three levels of quality of the movement; the analyzed metric is updated in real-time and shown on the screen; R1 processes a final report of the exercise which includes the information acquired during the interaction; R1 keeps engaging a user, proposing movements from the repertoire which have not been proposed before, by using face recognition and stored data related to past interactions; the physical interaction between R1 and the patient is limited to the motion of R1's head. Note The code implementing this application is tagged as v0.1.0 .","title":"Y1M5"},{"location":"Y1M5/#y1m5","text":"This application has been developed during the first semester of 2018 with the aim of starting implementing the features required to support R1 as artificial assistant of the physiotherapist. The fundamental aspects considered are the following: R1 engages the user, proposing a rehabilitative exercise of the upper limbs, chosen from a predefined repertoire; the exercise the user has to perform is shown on the screen, through a template skeleton; R1 analyzes in real-time the execution of the exercise, providing a verbal feedback limited to three levels of quality of the movement; the analyzed metric is updated in real-time and shown on the screen; R1 processes a final report of the exercise which includes the information acquired during the interaction; R1 keeps engaging a user, proposing movements from the repertoire which have not been proposed before, by using face recognition and stored data related to past interactions; the physical interaction between R1 and the patient is limited to the motion of R1's head. Note The code implementing this application is tagged as v0.1.0 .","title":"Y1M5"},{"location":"Y1Q3/","text":"Y1Q3 \u00b6 This application has been developed during the second semester of 2018, with the aim of significantly improving Y1M5 by considering the following aspects: increasing the motor functioning of R1: R1 physically shows to the patient the movement to replicate, removing the need of visualizing the movement on the screen. This improvement is intended to support the interaction between robot and patient, promoting the use of a robotic platform rather than a simple fixed camera, with the final aim of improving the quality of rehabilitation of a patient; extending the motion analysis to the end-point: the trajectory of the end-point (hand) is evaluated during a reaching task, including also the speed profile; extending the provided feedback: the feedback includes important properties of the movement, such as the speed and the range of motion, improving significantly the capability of correction. Including the mentioned point, the application considers the following steps: R1 engages the user, proposing a rehabilitative exercise of the upper limbs, chosen from a predefined repertoire; R1 physically shows the exercise the user has to perform ; R1 analyzes in real-time the execution of the exercise, providing an extended verbal feedback which includes the speed of the movement, the range of motion and how well the target has been reached ; the analyzed metric is updated in real-time and shown on the screen; R1 processes a final report of the exercise which includes the information acquired during the interaction; R1 keeps engaging a user, proposing movements from the repertoire which have not been proposed before, by using face recognition and stored data related to past interactions; Note The code implementing this application is tagged as v0.2.0 .","title":"Y1Q3"},{"location":"Y1Q3/#y1q3","text":"This application has been developed during the second semester of 2018, with the aim of significantly improving Y1M5 by considering the following aspects: increasing the motor functioning of R1: R1 physically shows to the patient the movement to replicate, removing the need of visualizing the movement on the screen. This improvement is intended to support the interaction between robot and patient, promoting the use of a robotic platform rather than a simple fixed camera, with the final aim of improving the quality of rehabilitation of a patient; extending the motion analysis to the end-point: the trajectory of the end-point (hand) is evaluated during a reaching task, including also the speed profile; extending the provided feedback: the feedback includes important properties of the movement, such as the speed and the range of motion, improving significantly the capability of correction. Including the mentioned point, the application considers the following steps: R1 engages the user, proposing a rehabilitative exercise of the upper limbs, chosen from a predefined repertoire; R1 physically shows the exercise the user has to perform ; R1 analyzes in real-time the execution of the exercise, providing an extended verbal feedback which includes the speed of the movement, the range of motion and how well the target has been reached ; the analyzed metric is updated in real-time and shown on the screen; R1 processes a final report of the exercise which includes the information acquired during the interaction; R1 keeps engaging a user, proposing movements from the repertoire which have not been proposed before, by using face recognition and stored data related to past interactions; Note The code implementing this application is tagged as v0.2.0 .","title":"Y1Q3"},{"location":"Y2Q2/","text":"Y2Q2 \u00b6 This application has been developed during the first semester of 2019, with the aim of enlarging the scenarios to include clinical tests (besides rehabilitation exercises as in Y1Q3 ). Such scenarios highlight the advantage of using a robot, given by the autonomous execution of the test (which is highly repetitive for physiotherapists) and the storage of quantitative results. We started with the Timed Up and Go (TUG), which measures the time the patient takes to get up from a chair, walk for 3 meters, turn around, go back to the chair and seat again. While it's easy for a physiotherapist to measure the execution time, it's much more complicated to evaluate limbs metrics during the test (such as range of motion, step length), unless of equipping the patient and the room with sensors. In such scenario, the robot has to: explain the test to the patient; naturally interact with the patient and reply to his / her questions; navigate in the environment; monitor the patient while doing the test; extract real-time metrics of the lower limbs and export them in a final report for further analysis.","title":"Y2Q2"},{"location":"Y2Q2/#y2q2","text":"This application has been developed during the first semester of 2019, with the aim of enlarging the scenarios to include clinical tests (besides rehabilitation exercises as in Y1Q3 ). Such scenarios highlight the advantage of using a robot, given by the autonomous execution of the test (which is highly repetitive for physiotherapists) and the storage of quantitative results. We started with the Timed Up and Go (TUG), which measures the time the patient takes to get up from a chair, walk for 3 meters, turn around, go back to the chair and seat again. While it's easy for a physiotherapist to measure the execution time, it's much more complicated to evaluate limbs metrics during the test (such as range of motion, step length), unless of equipping the patient and the room with sensors. In such scenario, the robot has to: explain the test to the patient; naturally interact with the patient and reply to his / her questions; navigate in the environment; monitor the patient while doing the test; extract real-time metrics of the lower limbs and export them in a final report for further analysis.","title":"Y2Q2"},{"location":"about/","text":"assistive-rehab has been developed within the Joint Lab between IIT and Fondazione Don Carlo Gnocchi Onlus . Maintainer \u00b6 Vasco Valentina ( @vvasco ) Contributors \u00b6 alphabetical order main components Ugo Pattacini ( @pattacini ) Skeletons Handling Vadim Tikhanoff ( @vtikha ) Face Recognition, I/F to OpenPose Valentina Vasco ( @vvasco ) Action Recognition, Movements Analysis Special Thanks \u00b6 alphabetical order reason Diego Ferigo ( @diegoferigo ) Hints on the website","title":"About"},{"location":"about/#maintainer","text":"Vasco Valentina ( @vvasco )","title":"Maintainer"},{"location":"about/#contributors","text":"alphabetical order main components Ugo Pattacini ( @pattacini ) Skeletons Handling Vadim Tikhanoff ( @vtikha ) Face Recognition, I/F to OpenPose Valentina Vasco ( @vvasco ) Action Recognition, Movements Analysis","title":"Contributors"},{"location":"about/#special-thanks","text":"alphabetical order reason Diego Ferigo ( @diegoferigo ) Hints on the website","title":"Special Thanks"},{"location":"align_signals/","text":"How to temporally align two signals \u00b6 Some applications require signals to be temporally aligned. For example, when analyzing and comparing the current and the template skeleton for producing a feedback, joints must be aligned. Given two 1D signals, whose temporal samples are stored in vectors v1 and v2 , you can use the following code snippet to get the aligned versions w1 and w2 : assistive_rehab :: Dtw dtw ( - 1 ); std :: vector < double > w_v1 , w_v2 ; dtw . align ( v1 , v2 , w_v1 , w_v2 ); double d = dtw . getDistance (); Tip In the example, no adjustment window condition is applied. Therefore the search of the warping path is done along the whole distance matrix. To limit the search, you can create the obejct dtw by passing the desired window, e.g. assistive_rehab::Dtw dtw(8) . The following images show two signals before and after the application of DTW: Before DTW After DTW For the multidimensional case, the code can be adapted as following: assistive_rehab :: Dtw dtw ( - 1 ); std :: vector < std :: vector < double >> w_v1 , w_v2 ; dtw . align ( v1 , v2 , w_v1 , w_v2 ); double d = dtw . getDistance (); Note v1 and v2 are defined as std::vector<std::vector> .","title":"How to temporally align two signals"},{"location":"align_signals/#how-to-temporally-align-two-signals","text":"Some applications require signals to be temporally aligned. For example, when analyzing and comparing the current and the template skeleton for producing a feedback, joints must be aligned. Given two 1D signals, whose temporal samples are stored in vectors v1 and v2 , you can use the following code snippet to get the aligned versions w1 and w2 : assistive_rehab :: Dtw dtw ( - 1 ); std :: vector < double > w_v1 , w_v2 ; dtw . align ( v1 , v2 , w_v1 , w_v2 ); double d = dtw . getDistance (); Tip In the example, no adjustment window condition is applied. Therefore the search of the warping path is done along the whole distance matrix. To limit the search, you can create the obejct dtw by passing the desired window, e.g. assistive_rehab::Dtw dtw(8) . The following images show two signals before and after the application of DTW: Before DTW After DTW For the multidimensional case, the code can be adapted as following: assistive_rehab :: Dtw dtw ( - 1 ); std :: vector < std :: vector < double >> w_v1 , w_v2 ; dtw . align ( v1 , v2 , w_v1 , w_v2 ); double d = dtw . getDistance (); Note v1 and v2 are defined as std::vector<std::vector> .","title":"How to temporally align two signals"},{"location":"comparison_releases/","text":"Comparison between Y1Q3 and Y1M5 \u00b6 Acquisition \u00b6 The improvements that Y1Q3 introduces in terms of acquisition are listed below: Filtering the disparity map ; Optimizing the skeleton ; Reducing latencies . Filtering the disparity map \u00b6 The disparity map provided by the camera is inaccurate around the human contour and might have holes, leading keypoints close to the contour (e.g. hands) or on an hole to be projected incorrectly. For example, the following video shows the depth map for an abduction movement, where the hand is projected to infinite: In Y1M5, the disparity map is used as provided by the camera, without any additional processing, and thus it is affected by the effect described. In Y1Q3, the disparity map is eroded (and thus the depth map dilated), which has the double effect of: increasing the probability for keypoints closer to the contour to fall within the correct depth map; filling holes in the depth. The following video shows the effect of filtering the depth map and keypoints falling within the correct depth: Unprocessed depth (Y1M5) Filtered depth (Y1Q3) Keypoints inside the depth Optimizing the skeleton \u00b6 Exercises that require movements parallel to the optical axis make some keypoints ambiguous. For example, in the external and internal rotation, the elbow is not directly observable as the hand occludes it. Therefore both keypoints are projected to the same depth, as shown here: Without optimization (Y1M5) In Y1Q3, we introduce an optimization of the skeleton, which adjusts the depth of the keypoints such that the length of the arms is equal to that observed during an initial phase. The following image shows the result of the optimization, in which elbow and hand are projected correctly. With optimization (Y1Q3) Reducing latencies \u00b6 yarpOpenPose introduces a visible latency in the depth map, as shown here: Without yarpOpenPose With yarpOpenPose In Y1Q3, yarpOpenPose propagates the depth image in sync with the output of skeleton detection, in order to equalize the delay between the two streams. Extending the feedback \u00b6 Y1M5 \u00b6 In Y1M5, the feedback is based on the definition of dynamic joints (i.e. joints performing the exercise (1)) and static joints (i.e. joints staying in the initial position (2)): dynamic joints contribute to the dynamic score, which computes their range of motion in a predefined temporal window, by awarding joints moving within the correct range of motion; static joints contribute to the static score, which computes their deviation from a predefined resting position. Dynamic and static scores are combined to produce three levels: low: both dynamic and static scores are low, producing the following feedback: \"You are not moving very well!\" medium: either dynamic or static score is medium, producing the following feedback: \"You are doing the exercise correctly, but you could do it better.\" high: both dynamic and static scores are high, producing the following feedback: \"You are moving very well!\" Failure the feedback does not provide a real correction of the movement; the speed of the movement is not directly taken into account; the feedback can be positive even when a wrong movement is performed (for example, a different movement with the same moving joints as external rotation instead of internal and viceversa, or a random movement with dynamic joints moving within the range of motion and static joints not deviating enough from the resting position). Y1Q3 \u00b6 To overcome the drawbacks of Y1M5, in Y1Q3 the concept of dynamic and static joints is removed and the exercise is treated in its entirety as an action. Therefore the feedback is produced based on the following two layers: Action recognition : which classifies the exercise and triggers the second layer if the predicted class equals the label of the exercise to perform. Otherwise the feedback produced is negative. This layer is fundamental as it guarantees a random or a wrong movement to not produce a positive feedback. Joint analysis : which compares the observed skeleton with a predefined template for that movement, differently based on the exercise. The comparison with a template skeleton allows us to analyze also the speed of the movement. With this architecture, the feedback is extended and articulated in the following levels: positive feedback: \"You are moving very well!\" feedback for range of motion exercises: feedback on the speed: \"Move the arm faster/slower!\" feedback on the range of motion: \"Move the arm further up/down!\" feedback for reaching exercises: \"You are not reaching the target!\" negative feedback: \"You are doing the wrong exercise. Please, repeat the movements I show you.\" Action Recognition \u00b6 The action recognition is carried out using a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) cells . Input to the network : 2D joints of the upper body from the skeletonRetriever . The output of the skeletonRetriever is preferable to that of yarpOpenPose , as the skeletonRetriever unambiguously identifies a skeleton by tag, avoiding ambiguities due to the presence of several skeletons. Preprocessing : The following operations are applied to the skeleton: normalization, such that the length of each segment is 1. This prevents having different results for different physiques; rototranslation, derived by the first observed skeleton; scale by a factor of 10, in order to have values between -1.0,1.0. Training set : 1 subject performs the following 6 exercises (i.e. 6 classes): abduction-left ; internal-rotation-left ; external-rotation-left ; reaching-left ; static (the subject remains steady); random (the subject moves randomly). The first 3 movements are repeated 10 times and the full exercise repeated 5 times. For the 4 th movement, 4 targets to be reached are defined, distributed on the corners/center of a square, centered around the shoulderLeft of the subject. Each dataset was recorded from a frontal and a side view and can be found at this link . Parameters used for training are the following: n_hidden = 22 n_steps (temporal window) = 30 learning_rate = 0.0005 batch_size = 256 epochs = 400 Validation set: The same subject, but previously unseen data, were used for testing the network. Accuracy: We get an accuracy of 92.2% with the following confusion matrix: Joint analysis \u00b6 This analysis is differentiated according to the exercises, which are classified as: range of motion exercises (i.e. abduction, internal and external rotation); reaching exercises. Range of Motion exercises \u00b6 The joints under analysis for these movements are: elbowLeft , handLeft . These movements can produce two feedbacks, i.e. on the speed and on the range of motion. The feedback is provided according to a predefined hierarchy which prioritizes the speed, followed by the range of motion. Therefore, a positive feedback is produced only when both checks are fine. Speed: Fourier analysis We perform the Fourier transform of each component of the joints under analysis in a predefined temporal window, for both the observed and the template skeleton. The difference in frequency is computed as df = f_skeleton - f_template and thus we can have two possible cases: df > 0 => feedback: \"Move the arm faster\" df < 0 => feedback: \"Move the arm slower\" Range of motion: Dynamic Time Warping (DTW) plus error statistical analysis The DTW is applied to each component of the joints under analysis, for both the observed and the template skeleton, allowing us to temporally align the signals to compare. Once joints are aligned, the error between the observed and template joints under analysis is computed. A statistical analysis is carried out, which looks for tails in the error distribution. Tails can be identified using the skewness of the distribution. Three cases can be identified: the skeleton and the template are moving similarly. Therefore the error in position is close to 0, generating a distribution centered around 0 (i.e. with low skewness): the skeleton has an higher range of motion than the template. Therefore the error will be positive, generating a distribution with a positive tail (i.e. with a positive skewness): the skeleton has a lower range of motion than the template. Therefore the error will be negative, generating a distribution with a negative tail (i.e. with a negative skewness): Reaching \u00b6 The joint under analysis for this movement is handLeft . This movement produces a feedback related to how well a predefined target is reached. Statistical analysis of the reached target We define a sphere around the predefined target and consider the points of the template which fall within the sphere. Statistics of the distribution of the template points are extracted to describe how the points should be distributed around the target. We then compute the number of points of the observed skeleton which fall within the template distribution. If the number of inliers is above a predefined threshold, the target is considered reached and a positive feedback is produced, otherwise the target is considered not reached. Comparison \u00b6 The following tables compare feedbacks produced by the two pipelines developed in Y1M5 and in Y1Q3 respectively, in response to the same movement. Corrrect feedbacks are highlighted. abduction-left Y1M5 Y1Q3 Correct 1. You are moving very well! 2. You are moving very well! 1. You are moving very well! 2. You are moving very well! Fast 1.You are moving very well! 2.You are moving very well! 1. Move the left arm slower! 2. Move the left arm slower! Slow 1.You are moving very well! 2.You are moving very well! 1. Move the left arm further up! 2. Move the left arm further up! Low ROM 1.You are moving very well! 2.You are moving very well! 1. Move the left arm further up! 2. Move the left arm further up! Wrong 1. You are doing the exercise correctly, but you could do it better. 2. You are doing the exercise correctly, but you could do it better. 1. You are doing the wrong exercise. 2. You are doing the wrong exercise. external-rotation-left Y1M5 Y1Q3 Correct 1. You are doing the exercise correctly, but you could do it better. 2. You are doing the exercise correctly, but you could do it better. 1. Move the left arm slower! 2.You are moving very well! Fast 1.You are moving very well! 2.You are moving very well! 1.Move the left arm slower! 2.Move the left arm slower! Slow 1. You are doing the exercise correctly, but you could do it better. 2. You are moving very well! 1. You are moving very well! 2. Move the left arm faster! Low ROM 1.You are moving very well! 2. You are doing the exercise correctly, but you could do it better. 1.You are moving very well! 2.Move the left arm slower! Wrong 1. You are doing the exercise correctly, but you could do it better. 2. You are doing the exercise correctly, but you could do it better. 1. You are doing the wrong exercise. 2. You are doing the wrong exercise. internal-rotation-left Y1M5 Y1Q3 Correct 1.You are moving very well! 2.You are moving very well! 1.You are moving very well! 2.You are moving very well! Fast 1.You are moving very well! 2.You are moving very well! 1.Move the left arm slower! 2.Move the left arm slower! Slow 1.You are moving very well! 2.You are moving very well! 1.You are moving very well! 2.Move the left arm faster! Low ROM 1.You are not moving very well! 2.You are not moving very well! 1. Move the left arm backwards! 2. Move the left arm backwards! Wrong 1.You are not moving very well! 2.You are not moving very well! 1. You are doing the wrong exercise. 2. You are doing the wrong exercise. Success in Y1Q3 the correct feedback is provided for fast movements, while in Y1M5 the feedback is always positive, regardless the speed; in Y1Q3 the correct feedback is provided for low ROM movements, while in Y1M5 the feedback is always positive (except for the external rotation, which generates a medium score since there are time instants where the range of motion is below the minimum one); in Y1Q3 a negative feedback is provided for wrong movements, while in Y1M5 the feedback is generated by a medium score, (except for the internal rotation, due to the static score, which is low as static joints deviate a lot from the resting position). Failure in Y1Q3 a positive feedback is provided for slow movements. This occurs as the template skeleton moves slowly to emulate the movement shown by the robot and the feedback on the speed is triggered only by a very slow execution of the exercise; in Y1Q3 a feedback on the speed is provided for an external rotation with low ROM , as the threshold on the frequency is set low to trigger also a feedback on slow movements.","title":"Comparison between Y1Q3 and Y1M5"},{"location":"comparison_releases/#comparison-between-y1q3-and-y1m5","text":"","title":"Comparison between Y1Q3 and Y1M5"},{"location":"comparison_releases/#acquisition","text":"The improvements that Y1Q3 introduces in terms of acquisition are listed below: Filtering the disparity map ; Optimizing the skeleton ; Reducing latencies .","title":"Acquisition"},{"location":"comparison_releases/#filtering-the-disparity-map","text":"The disparity map provided by the camera is inaccurate around the human contour and might have holes, leading keypoints close to the contour (e.g. hands) or on an hole to be projected incorrectly. For example, the following video shows the depth map for an abduction movement, where the hand is projected to infinite: In Y1M5, the disparity map is used as provided by the camera, without any additional processing, and thus it is affected by the effect described. In Y1Q3, the disparity map is eroded (and thus the depth map dilated), which has the double effect of: increasing the probability for keypoints closer to the contour to fall within the correct depth map; filling holes in the depth. The following video shows the effect of filtering the depth map and keypoints falling within the correct depth: Unprocessed depth (Y1M5) Filtered depth (Y1Q3) Keypoints inside the depth","title":"Filtering the disparity map"},{"location":"comparison_releases/#optimizing-the-skeleton","text":"Exercises that require movements parallel to the optical axis make some keypoints ambiguous. For example, in the external and internal rotation, the elbow is not directly observable as the hand occludes it. Therefore both keypoints are projected to the same depth, as shown here: Without optimization (Y1M5) In Y1Q3, we introduce an optimization of the skeleton, which adjusts the depth of the keypoints such that the length of the arms is equal to that observed during an initial phase. The following image shows the result of the optimization, in which elbow and hand are projected correctly. With optimization (Y1Q3)","title":"Optimizing the skeleton"},{"location":"comparison_releases/#reducing-latencies","text":"yarpOpenPose introduces a visible latency in the depth map, as shown here: Without yarpOpenPose With yarpOpenPose In Y1Q3, yarpOpenPose propagates the depth image in sync with the output of skeleton detection, in order to equalize the delay between the two streams.","title":"Reducing latencies"},{"location":"comparison_releases/#extending-the-feedback","text":"","title":"Extending the feedback"},{"location":"comparison_releases/#y1m5","text":"In Y1M5, the feedback is based on the definition of dynamic joints (i.e. joints performing the exercise (1)) and static joints (i.e. joints staying in the initial position (2)): dynamic joints contribute to the dynamic score, which computes their range of motion in a predefined temporal window, by awarding joints moving within the correct range of motion; static joints contribute to the static score, which computes their deviation from a predefined resting position. Dynamic and static scores are combined to produce three levels: low: both dynamic and static scores are low, producing the following feedback: \"You are not moving very well!\" medium: either dynamic or static score is medium, producing the following feedback: \"You are doing the exercise correctly, but you could do it better.\" high: both dynamic and static scores are high, producing the following feedback: \"You are moving very well!\" Failure the feedback does not provide a real correction of the movement; the speed of the movement is not directly taken into account; the feedback can be positive even when a wrong movement is performed (for example, a different movement with the same moving joints as external rotation instead of internal and viceversa, or a random movement with dynamic joints moving within the range of motion and static joints not deviating enough from the resting position).","title":"Y1M5"},{"location":"comparison_releases/#y1q3","text":"To overcome the drawbacks of Y1M5, in Y1Q3 the concept of dynamic and static joints is removed and the exercise is treated in its entirety as an action. Therefore the feedback is produced based on the following two layers: Action recognition : which classifies the exercise and triggers the second layer if the predicted class equals the label of the exercise to perform. Otherwise the feedback produced is negative. This layer is fundamental as it guarantees a random or a wrong movement to not produce a positive feedback. Joint analysis : which compares the observed skeleton with a predefined template for that movement, differently based on the exercise. The comparison with a template skeleton allows us to analyze also the speed of the movement. With this architecture, the feedback is extended and articulated in the following levels: positive feedback: \"You are moving very well!\" feedback for range of motion exercises: feedback on the speed: \"Move the arm faster/slower!\" feedback on the range of motion: \"Move the arm further up/down!\" feedback for reaching exercises: \"You are not reaching the target!\" negative feedback: \"You are doing the wrong exercise. Please, repeat the movements I show you.\"","title":"Y1Q3"},{"location":"comparison_releases/#action-recognition","text":"The action recognition is carried out using a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) cells . Input to the network : 2D joints of the upper body from the skeletonRetriever . The output of the skeletonRetriever is preferable to that of yarpOpenPose , as the skeletonRetriever unambiguously identifies a skeleton by tag, avoiding ambiguities due to the presence of several skeletons. Preprocessing : The following operations are applied to the skeleton: normalization, such that the length of each segment is 1. This prevents having different results for different physiques; rototranslation, derived by the first observed skeleton; scale by a factor of 10, in order to have values between -1.0,1.0. Training set : 1 subject performs the following 6 exercises (i.e. 6 classes): abduction-left ; internal-rotation-left ; external-rotation-left ; reaching-left ; static (the subject remains steady); random (the subject moves randomly). The first 3 movements are repeated 10 times and the full exercise repeated 5 times. For the 4 th movement, 4 targets to be reached are defined, distributed on the corners/center of a square, centered around the shoulderLeft of the subject. Each dataset was recorded from a frontal and a side view and can be found at this link . Parameters used for training are the following: n_hidden = 22 n_steps (temporal window) = 30 learning_rate = 0.0005 batch_size = 256 epochs = 400 Validation set: The same subject, but previously unseen data, were used for testing the network. Accuracy: We get an accuracy of 92.2% with the following confusion matrix:","title":"Action Recognition"},{"location":"comparison_releases/#joint-analysis","text":"This analysis is differentiated according to the exercises, which are classified as: range of motion exercises (i.e. abduction, internal and external rotation); reaching exercises.","title":"Joint analysis"},{"location":"comparison_releases/#range-of-motion-exercises","text":"The joints under analysis for these movements are: elbowLeft , handLeft . These movements can produce two feedbacks, i.e. on the speed and on the range of motion. The feedback is provided according to a predefined hierarchy which prioritizes the speed, followed by the range of motion. Therefore, a positive feedback is produced only when both checks are fine. Speed: Fourier analysis We perform the Fourier transform of each component of the joints under analysis in a predefined temporal window, for both the observed and the template skeleton. The difference in frequency is computed as df = f_skeleton - f_template and thus we can have two possible cases: df > 0 => feedback: \"Move the arm faster\" df < 0 => feedback: \"Move the arm slower\" Range of motion: Dynamic Time Warping (DTW) plus error statistical analysis The DTW is applied to each component of the joints under analysis, for both the observed and the template skeleton, allowing us to temporally align the signals to compare. Once joints are aligned, the error between the observed and template joints under analysis is computed. A statistical analysis is carried out, which looks for tails in the error distribution. Tails can be identified using the skewness of the distribution. Three cases can be identified: the skeleton and the template are moving similarly. Therefore the error in position is close to 0, generating a distribution centered around 0 (i.e. with low skewness): the skeleton has an higher range of motion than the template. Therefore the error will be positive, generating a distribution with a positive tail (i.e. with a positive skewness): the skeleton has a lower range of motion than the template. Therefore the error will be negative, generating a distribution with a negative tail (i.e. with a negative skewness):","title":"Range of Motion exercises"},{"location":"comparison_releases/#reaching","text":"The joint under analysis for this movement is handLeft . This movement produces a feedback related to how well a predefined target is reached. Statistical analysis of the reached target We define a sphere around the predefined target and consider the points of the template which fall within the sphere. Statistics of the distribution of the template points are extracted to describe how the points should be distributed around the target. We then compute the number of points of the observed skeleton which fall within the template distribution. If the number of inliers is above a predefined threshold, the target is considered reached and a positive feedback is produced, otherwise the target is considered not reached.","title":"Reaching"},{"location":"comparison_releases/#comparison","text":"The following tables compare feedbacks produced by the two pipelines developed in Y1M5 and in Y1Q3 respectively, in response to the same movement. Corrrect feedbacks are highlighted. abduction-left Y1M5 Y1Q3 Correct 1. You are moving very well! 2. You are moving very well! 1. You are moving very well! 2. You are moving very well! Fast 1.You are moving very well! 2.You are moving very well! 1. Move the left arm slower! 2. Move the left arm slower! Slow 1.You are moving very well! 2.You are moving very well! 1. Move the left arm further up! 2. Move the left arm further up! Low ROM 1.You are moving very well! 2.You are moving very well! 1. Move the left arm further up! 2. Move the left arm further up! Wrong 1. You are doing the exercise correctly, but you could do it better. 2. You are doing the exercise correctly, but you could do it better. 1. You are doing the wrong exercise. 2. You are doing the wrong exercise. external-rotation-left Y1M5 Y1Q3 Correct 1. You are doing the exercise correctly, but you could do it better. 2. You are doing the exercise correctly, but you could do it better. 1. Move the left arm slower! 2.You are moving very well! Fast 1.You are moving very well! 2.You are moving very well! 1.Move the left arm slower! 2.Move the left arm slower! Slow 1. You are doing the exercise correctly, but you could do it better. 2. You are moving very well! 1. You are moving very well! 2. Move the left arm faster! Low ROM 1.You are moving very well! 2. You are doing the exercise correctly, but you could do it better. 1.You are moving very well! 2.Move the left arm slower! Wrong 1. You are doing the exercise correctly, but you could do it better. 2. You are doing the exercise correctly, but you could do it better. 1. You are doing the wrong exercise. 2. You are doing the wrong exercise. internal-rotation-left Y1M5 Y1Q3 Correct 1.You are moving very well! 2.You are moving very well! 1.You are moving very well! 2.You are moving very well! Fast 1.You are moving very well! 2.You are moving very well! 1.Move the left arm slower! 2.Move the left arm slower! Slow 1.You are moving very well! 2.You are moving very well! 1.You are moving very well! 2.Move the left arm faster! Low ROM 1.You are not moving very well! 2.You are not moving very well! 1. Move the left arm backwards! 2. Move the left arm backwards! Wrong 1.You are not moving very well! 2.You are not moving very well! 1. You are doing the wrong exercise. 2. You are doing the wrong exercise. Success in Y1Q3 the correct feedback is provided for fast movements, while in Y1M5 the feedback is always positive, regardless the speed; in Y1Q3 the correct feedback is provided for low ROM movements, while in Y1M5 the feedback is always positive (except for the external rotation, which generates a medium score since there are time instants where the range of motion is below the minimum one); in Y1Q3 a negative feedback is provided for wrong movements, while in Y1M5 the feedback is generated by a medium score, (except for the internal rotation, due to the static score, which is low as static joints deviate a lot from the resting position). Failure in Y1Q3 a positive feedback is provided for slow movements. This occurs as the template skeleton moves slowly to emulate the movement shown by the robot and the feedback on the speed is triggered only by a very slow execution of the exercise; in Y1Q3 a feedback on the speed is provided for an external rotation with low ROM , as the threshold on the frequency is set low to trigger also a feedback on slow movements.","title":"Comparison"},{"location":"create_new_skeleton/","text":"How to manage a skeleton object \u00b6 This tutorial will explain you the main functionalities to manage a skeleton object for: defining a skeleton; retrieving a skeleton; accessing single keypoints of a skeleton; dealing with pixels; additional functionalities. Defining a skeleton \u00b6 A skeleton can be defined as: unordered list of keypoints : the list defines the keypoints as pairs of points associated to the keypoint tag; ordered list of keypoints : the list defines the keypoints according to the following order: 0: shoulder_center 1: head 2: shoulder_left 3: elbow_left 4: hand_left 5: shoulder_right 6: elbow_right 7: hand_right 8: hip_center 9: hip_left 10: knee_left 11: ankle_left 12: foot_left 13: hip_right 14: knee_right 15: ankle_right 16: foot_right property-like structure : the available properties are the following: type: string containing skeleton's type (\"assistive_rehab::SkeletonStd\"). tag: string containing skeleton's tag. transformation: 4 x 4 skeleton's roto-translation matrix. coronal: vector containing skeleton's coronal plane. sagittal: vector containing skeleton's sagittal plane. transverse: vector containing skeleton's transverse plane. skeleton: list containing keypoints with the following subproperties: tag: string containing keypoint's tag. status: string containing keypoint's status (updated or stale). position: vector containing keypoint's camera coordinates x,y,z. pixel: vector containing keypoint's image coordinates u,v. child: list containing keypoint's child, specified as position, status, tag. An example is the following: (coronal (-0.0171187190241832 0.168776145353523 -0.984581522687332)) (sagittal (0.999852451902468 0.0040979367701588 -0.0166817666585492)) (skeleton (((child (((pixel (143.0 18.0)) (position (-0.0958624515848492 -0.655793450456223 2.01140785217285)) (status updated) (tag head)) ((child (((child (((pixel (182.0 106.0)) (position (0.132475296984726 -0.0970469850595488 2.24838447570801)) (status updated) (tag handLeft)))) (pixel (179.0 77.0)) (position (0.126047049721491 -0.301864959631551 2.20449256896973)) (status updated) (tag elbowLeft)))) (pixel (173.0 40.0)) (position (0.0829495294433831 -0.549497736338085 2.17046546936035)) (status updated) (tag shoulderLeft)) ((child (((child (((pixel (117.0 102.0)) (position (-0.318757589652556 -0.129758383247854 2.30834770202637)) (status updated) (tag handRight)))) (pixel (124.0 75.0)) (position (-0.265819393982889 -0.319631634579399 2.22591972351074)) (status updated) (tag elbowRight)))) (pixel (132.0 42.0)) (position (-0.194277581569445 -0.550633963158095 2.17509078979492)) (status updated) (tag shoulderRight)) ((child (((child (((child (((child (((pixel (153.0 200.0)) (position (-0.0305329581785242 0.615750619982796 2.40194511413574)) (status updated) (tag footLeft)))) (pixel (156.0 187.0)) (position (-0.0161527965668162 0.538963623868779 2.52594947814941)) (status updated) (tag ankleLeft)))) (pixel (158.0 148.0)) (position (-0.00361483866030924 0.218990793550177 2.42797660827637)) (status updated) (tag kneeLeft)))) (pixel (164.0 105.0)) (position (0.0249104292284522 -0.102991968370345 2.2304573059082)) (status updated) (tag hipLeft)) ((child (((child (((child (((pixel (138.0 194.0)) (position (-0.170998545149134 0.592533010093213 2.49751472473145)) (status updated) (tag footRight)))) (pixel (144.0 182.0)) (position (-0.130142124436922 0.509498389750101 2.55434036254883)) (status updated) (tag ankleRight)))) (pixel (142.0 147.0)) (position (-0.133305882322194 0.209480672220591 2.39202308654785)) (status updated) (tag kneeRight)))) (pixel (138.0 103.0)) (position (-0.158599348153779 -0.109193487775799 2.21346664428711)) (status updated) (tag hipRight)))) (pixel (151.0 103.0)) (position (-0.0738603465810386 -0.102184160595443 2.20890045166016)) (status updated) (tag hipCenter)))) (pixel (153.0 41.0)) (position (-0.054339762871519 -0.540136057902244 2.13348770141602)) (status updated) (tag shoulderCenter)))) (tag \"#7\") (transformation (4 4 (1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0))) (transverse (0.0438836580749896 -0.984546958261919 -0.169533216601232)) (type \"assistive_rehab::SkeletonStd\") From an unordered list of keypoints \u00b6 The following code snippet creates a SkeletonStd object from an unordered list of keypoints: #include <yarp/sig/Vector.h> #include \"AssistiveRehab/skeleton.h\" int main () { assistive_rehab :: SkeletonStd skeleton ; std :: vector < std :: pair < std :: string , yarp :: sig :: Vector >> unordered ; { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.0 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: shoulder_center , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.0 ; p [ 1 ] =- 0.1 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: head , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.1 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: shoulder_left , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.2 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: elbow_left , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.3 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: hand_left , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.1 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: shoulder_right , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.2 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: elbow_right , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.3 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: hand_right , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.0 ; p [ 1 ] = 0.1 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: hip_center , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.1 ; p [ 1 ] = 0.1 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: hip_left , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.1 ; p [ 1 ] = 0.2 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: knee_left , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.1 ; p [ 1 ] = 0.1 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: hip_right , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.1 ; p [ 1 ] = 0.2 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: knee_right , p )); } skeleton . setTag ( \"unordered\" ); skeleton . update ( unordered ); skeleton . print (); return EXIT_SUCCESS ; } The result is: tag = \"unordered\" transformation = -1.000 0.000 0.000 0.000 0.000 1.000 0.000 0.000 -0.000 0.000 -1.000 0.000 0.000 0.000 0.000 1.000 coronal = (-0.000 0.000 1.000) sagittal = (-1.000 0.000 -0.000) transverse = ( 0.000 -1.000 0.000) keypoint[\"shoulderCenter\"] = ( 0.000 0.000 0.000); pixel=( nan nan); status=updated; parent={}; child={\"head\" \"shoulderLeft\" \"shoulderRight\" \"hipCenter\" } keypoint[\"head\"] = ( 0.000 -0.100 0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={} keypoint[\"shoulderLeft\"] = (-0.100 0.000 -0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={\"elbowLeft\" } keypoint[\"elbowLeft\"] = (-0.200 0.000 -0.000); pixel=( nan nan); status=updated; parent={\"shoulderLeft\" }; child={\"handLeft\" } keypoint[\"handLeft\"] = (-0.300 0.000 -0.000); pixel=( nan nan); status=updated; parent={\"elbowLeft\" }; child={} keypoint[\"shoulderRight\"] = ( 0.100 0.000 0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={\"elbowRight\" } keypoint[\"elbowRight\"] = ( 0.200 0.000 0.000); pixel=( nan nan); status=updated; parent={\"shoulderRight\" }; child={\"handRight\" } keypoint[\"handRight\"] = ( 0.300 0.000 0.000); pixel=( nan nan); status=updated; parent={\"elbowRight\" }; child={} keypoint[\"hipCenter\"] = ( 0.000 0.100 0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={\"hipLeft\" \"hipRight\" } keypoint[\"hipLeft\"] = (-0.100 0.100 -0.000); pixel=( nan nan); status=updated; parent={\"hipCenter\" }; child={\"kneeLeft\" } keypoint[\"kneeLeft\"] = (-0.100 0.200 -0.000); pixel=( nan nan); status=updated; parent={\"hipLeft\" }; child={\"ankleLeft\" } keypoint[\"ankleLeft\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"kneeLeft\" }; child={\"footLeft\" } keypoint[\"footLeft\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"ankleLeft\" }; child={} keypoint[\"hipRight\"] = ( 0.100 0.100 0.000); pixel=( nan nan); status=updated; parent={\"hipCenter\" }; child={\"kneeRight\" } keypoint[\"kneeRight\"] = ( 0.100 0.200 0.000); pixel=( nan nan); status=updated; parent={\"hipRight\" }; child={\"ankleRight\" } keypoint[\"ankleRight\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"kneeRight\" }; child={\"footRight\" } keypoint[\"footRight\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"ankleRight\" }; child={} Note Don't get worried about nan in the pixel fields: it's correct since we didn't set them up yet. Uninitialized points and/or pixels take nan values. From an ordered list of keypoints \u00b6 The following code snippet creates a SkeletonStd object from an ordered list of keypoints: #include <yarp/sig/Vector.h> #include \"AssistiveRehab/skeleton.h\" int main () { assistive_rehab :: SkeletonStd skeleton ; std :: vector < yarp :: sig :: Vector > ordered ; { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.0 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.0 ; p [ 1 ] =- 0.1 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.1 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.2 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.3 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.1 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.2 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.3 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.1 ; p [ 1 ] = 0.1 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.1 ; p [ 1 ] = 0.2 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.1 ; p [ 1 ] = 0.1 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.1 ; p [ 1 ] = 0.2 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } skeleton . setTag ( \"ordered\" ); skeleton . update ( ordered ); skeleton . print (); return EXIT_SUCCESS ; } The result is: tag = \"ordered\" transformation = -1.000 0.000 0.000 0.000 0.000 1.000 0.000 0.000 -0.000 0.000 -1.000 0.000 0.000 0.000 0.000 1.000 coronal = (-0.000 0.000 0.707) sagittal = (-1.000 0.000 -0.000) transverse = ( 0.707 -0.707 0.000) keypoint[\"shoulderCenter\"] = ( 0.000 0.000 0.000); pixel=( nan nan); status=updated; parent={}; child={\"head\" \"shoulderLeft\" \"shoulderRight\" \"hipCenter\" } keypoint[\"head\"] = ( 0.000 -0.100 0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={} keypoint[\"shoulderLeft\"] = (-0.100 0.000 -0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={\"elbowLeft\" } keypoint[\"elbowLeft\"] = (-0.200 0.000 -0.000); pixel=( nan nan); status=updated; parent={\"shoulderLeft\" }; child={\"handLeft\" } keypoint[\"handLeft\"] = (-0.300 0.000 -0.000); pixel=( nan nan); status=updated; parent={\"elbowLeft\" }; child={} keypoint[\"shoulderRight\"] = ( 0.100 0.000 0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={\"elbowRight\" } keypoint[\"elbowRight\"] = ( 0.200 0.000 0.000); pixel=( nan nan); status=updated; parent={\"shoulderRight\" }; child={\"handRight\" } keypoint[\"handRight\"] = ( 0.300 0.000 0.000); pixel=( nan nan); status=updated; parent={\"elbowRight\" }; child={} keypoint[\"hipCenter\"] = (-0.100 0.100 -0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={\"hipLeft\" \"hipRight\" } keypoint[\"hipLeft\"] = (-0.100 0.200 -0.000); pixel=( nan nan); status=updated; parent={\"hipCenter\" }; child={\"kneeLeft\" } keypoint[\"kneeLeft\"] = ( 0.100 0.100 0.000); pixel=( nan nan); status=updated; parent={\"hipLeft\" }; child={\"ankleLeft\" } keypoint[\"ankleLeft\"] = ( 0.100 0.200 0.000); pixel=( nan nan); status=updated; parent={\"kneeLeft\" }; child={\"footLeft\" } keypoint[\"footLeft\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"ankleLeft\" }; child={} keypoint[\"hipRight\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"hipCenter\" }; child={\"kneeRight\" } keypoint[\"kneeRight\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"hipRight\" }; child={\"ankleRight\" } keypoint[\"ankleRight\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"kneeRight\" }; child={\"footRight\" } keypoint[\"footRight\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"ankleRight\" }; child={} From a property-like structure \u00b6 Assuming that the object skeleton1 has been previously defined using one of the instructions above, a new Skeleton object can be defined as following: yarp :: os :: Property prop = skeleton1 . toProperty (); std :: unique_ptr < assistive_rehab :: Skeleton > skeleton2 ( assistive_rehab :: skeleton_factory ( prop )); skeleton2 -> setTag ( \"properties\" ); skeleton2 -> print (); Retrieving a skeleton \u00b6 A skeleton can be retrieved as a property-like structure from a yarp port: assistive_rehab :: SkeletonStd skeleton ; if ( yarp :: os :: Bottle * b = opcPort . read ( false )) { yarp :: os :: Property prop ; prop . fromString ( b -> get ( 0 ). asList () -> toString ()); skeleton . update ( assistive_rehab :: skeleton_factory ( prop ) -> toProperty ()); } or from the OPC as following: assistive_rehab :: SkeletonStd skeleton ; yarp :: os :: Bottle cmd , reply ; cmd . addVocab ( yarp :: os :: Vocab :: encode ( \"ask\" )); yarp :: os :: Bottle & content = cmd . addList (). addList (); content . addString ( \"skeleton\" ); opcPort . write ( cmd , reply ); if ( reply . size () > 1 ) { if ( reply . get ( 0 ). asVocab () == Vocab :: encode ( \"ack\" )) { if ( yarp :: os :: Bottle * idField = reply . get ( 1 ). asList ()) { if ( yarp :: os :: Bottle * idValues = idField -> get ( 1 ). asList ()) { int id = idValues -> get ( 0 ). asInt (); cmd . clear (); cmd . addVocab ( Vocab :: encode ( \"get\" )); yarp :: os :: Bottle & content = cmd . addList (). addList (); yarp :: os :: Bottle replyProp ; content . addString ( \"id\" ); content . addInt ( id ); opcPort . write ( cmd , replyProp ); if ( replyProp . get ( 0 ). asVocab () == yarp :: os :: Vocab :: encode ( \"ack\" )) { if ( yarp :: os :: Bottle * propField = replyProp . get ( 1 ). asList ()) { yarp :: os :: Property prop ( propField -> toString (). c_str ()); skeleton . update ( assistive_rehab :: skeleton_factory ( prop ) -> toProperty ()); } } } } } } Accessing single keypoints of a skeleton \u00b6 If you want to access to a keypoint of the skeleton, you can use the operator[] of the class Skeleton , by passing as parameter the keypoint's tag. For example, the following snippet allows you to get the shoulder_center 3D coordinates: yarp :: sig :: Vector sc = skeleton [ assistive_rehab :: KeyPointTag :: shoulder_center ] -> getPoint (); Dealing with pixels \u00b6 Occasionally, it might be worth storing also the pixels alongside the points, which are used by the algorithm to reconstruct the 3D skeleton. This is particularly useful when the skeleton is employed to enable gaze tracking, for example. In this context, the 3D information of the keypoints needs normally to be transformed from the camera frame to the root frame of the robot. Instead, having the pixels would ease this process. The updating methods described above do have their pixel-wise counterparts: std :: vector < std :: pair < yarp :: sig :: Vector , yarp :: sig :: Vector >> ordered_withpixels ; yarp :: sig :: Vector p ( 3 , 0.1 ); yarp :: sig :: Vector px ( 2 , 10.0 ); ordered_withpixels . push_back ( std :: make_pair ( p , px )); skeleton . update_withpixels ( ordered_withpixels ); std :: vector < std :: pair < std :: string < std :: pair < yarp :: sig :: Vector , yarp :: sig :: Vector >>> unordered_withpixels ; yarp :: sig :: Vector p ( 3 , 0.1 ); yarp :: sig :: Vector px ( 2 , 10.0 ); unordered_withpixels . push_back ( std :: make_pair ( assistive_rehab :: KeyPointTag :: shoulder_center , std :: make_pair ( p , px ))); skeleton . update_withpixels ( unordered_withpixels ); yarp :: sig :: Vector pixel = skeleton [ assistive_rehab :: KeyPointTag :: shoulder_center ]. getPixel (); Additional functionalities \u00b6 Normalization \u00b6 Some applications might require a normalized skeleton, to avoid having different results for different human physiques. The normalization provided in the library makes the length of the observed human segments always equal to 1 and can be applied as following: skeleton . normalize (); Keypoints' reference system \u00b6 Keypoints in the skeleton are defined with respect to the camera. To change the reference system, given a transformation matrix T , you can use the method setTransformation , as following: skeleton . setTransformation ( T ); Tip If you want to use the skeleton as reference system, you can create the transformation matrix T from the skeleton's planes: yarp :: sig :: Vector coronal = skeleton . getCoronal (); yarp :: sig :: Vector sagittal = skeleton . getSagittal (); yarp :: sig :: Vector transverse = skeleton . getTransverse (); yarp :: sig :: Vector p = skeleton [ assistive_rehab :: KeyPointTag :: shoulder_center ] -> getPoint (); yarp :: sig :: Matrix T1 ( 4 , 4 ); T1 . setSubcol ( coronal , 0 , 0 ); T1 . setSubcol ( sagittal , 0 , 1 ); T1 . setSubcol ( transverse , 0 , 2 ); T1 . setSubcol ( p , 0 , 3 ); T1 ( 3 , 3 ) = 1.0 ; T = yarp :: math :: SE3inv ( T1 ); skeleton . setTransformation ( T );","title":"How to manage a skeleton object"},{"location":"create_new_skeleton/#how-to-manage-a-skeleton-object","text":"This tutorial will explain you the main functionalities to manage a skeleton object for: defining a skeleton; retrieving a skeleton; accessing single keypoints of a skeleton; dealing with pixels; additional functionalities.","title":"How to manage a skeleton object"},{"location":"create_new_skeleton/#defining-a-skeleton","text":"A skeleton can be defined as: unordered list of keypoints : the list defines the keypoints as pairs of points associated to the keypoint tag; ordered list of keypoints : the list defines the keypoints according to the following order: 0: shoulder_center 1: head 2: shoulder_left 3: elbow_left 4: hand_left 5: shoulder_right 6: elbow_right 7: hand_right 8: hip_center 9: hip_left 10: knee_left 11: ankle_left 12: foot_left 13: hip_right 14: knee_right 15: ankle_right 16: foot_right property-like structure : the available properties are the following: type: string containing skeleton's type (\"assistive_rehab::SkeletonStd\"). tag: string containing skeleton's tag. transformation: 4 x 4 skeleton's roto-translation matrix. coronal: vector containing skeleton's coronal plane. sagittal: vector containing skeleton's sagittal plane. transverse: vector containing skeleton's transverse plane. skeleton: list containing keypoints with the following subproperties: tag: string containing keypoint's tag. status: string containing keypoint's status (updated or stale). position: vector containing keypoint's camera coordinates x,y,z. pixel: vector containing keypoint's image coordinates u,v. child: list containing keypoint's child, specified as position, status, tag. An example is the following: (coronal (-0.0171187190241832 0.168776145353523 -0.984581522687332)) (sagittal (0.999852451902468 0.0040979367701588 -0.0166817666585492)) (skeleton (((child (((pixel (143.0 18.0)) (position (-0.0958624515848492 -0.655793450456223 2.01140785217285)) (status updated) (tag head)) ((child (((child (((pixel (182.0 106.0)) (position (0.132475296984726 -0.0970469850595488 2.24838447570801)) (status updated) (tag handLeft)))) (pixel (179.0 77.0)) (position (0.126047049721491 -0.301864959631551 2.20449256896973)) (status updated) (tag elbowLeft)))) (pixel (173.0 40.0)) (position (0.0829495294433831 -0.549497736338085 2.17046546936035)) (status updated) (tag shoulderLeft)) ((child (((child (((pixel (117.0 102.0)) (position (-0.318757589652556 -0.129758383247854 2.30834770202637)) (status updated) (tag handRight)))) (pixel (124.0 75.0)) (position (-0.265819393982889 -0.319631634579399 2.22591972351074)) (status updated) (tag elbowRight)))) (pixel (132.0 42.0)) (position (-0.194277581569445 -0.550633963158095 2.17509078979492)) (status updated) (tag shoulderRight)) ((child (((child (((child (((child (((pixel (153.0 200.0)) (position (-0.0305329581785242 0.615750619982796 2.40194511413574)) (status updated) (tag footLeft)))) (pixel (156.0 187.0)) (position (-0.0161527965668162 0.538963623868779 2.52594947814941)) (status updated) (tag ankleLeft)))) (pixel (158.0 148.0)) (position (-0.00361483866030924 0.218990793550177 2.42797660827637)) (status updated) (tag kneeLeft)))) (pixel (164.0 105.0)) (position (0.0249104292284522 -0.102991968370345 2.2304573059082)) (status updated) (tag hipLeft)) ((child (((child (((child (((pixel (138.0 194.0)) (position (-0.170998545149134 0.592533010093213 2.49751472473145)) (status updated) (tag footRight)))) (pixel (144.0 182.0)) (position (-0.130142124436922 0.509498389750101 2.55434036254883)) (status updated) (tag ankleRight)))) (pixel (142.0 147.0)) (position (-0.133305882322194 0.209480672220591 2.39202308654785)) (status updated) (tag kneeRight)))) (pixel (138.0 103.0)) (position (-0.158599348153779 -0.109193487775799 2.21346664428711)) (status updated) (tag hipRight)))) (pixel (151.0 103.0)) (position (-0.0738603465810386 -0.102184160595443 2.20890045166016)) (status updated) (tag hipCenter)))) (pixel (153.0 41.0)) (position (-0.054339762871519 -0.540136057902244 2.13348770141602)) (status updated) (tag shoulderCenter)))) (tag \"#7\") (transformation (4 4 (1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0))) (transverse (0.0438836580749896 -0.984546958261919 -0.169533216601232)) (type \"assistive_rehab::SkeletonStd\")","title":"Defining a skeleton"},{"location":"create_new_skeleton/#from-an-unordered-list-of-keypoints","text":"The following code snippet creates a SkeletonStd object from an unordered list of keypoints: #include <yarp/sig/Vector.h> #include \"AssistiveRehab/skeleton.h\" int main () { assistive_rehab :: SkeletonStd skeleton ; std :: vector < std :: pair < std :: string , yarp :: sig :: Vector >> unordered ; { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.0 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: shoulder_center , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.0 ; p [ 1 ] =- 0.1 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: head , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.1 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: shoulder_left , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.2 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: elbow_left , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.3 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: hand_left , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.1 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: shoulder_right , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.2 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: elbow_right , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.3 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: hand_right , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.0 ; p [ 1 ] = 0.1 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: hip_center , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.1 ; p [ 1 ] = 0.1 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: hip_left , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.1 ; p [ 1 ] = 0.2 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: knee_left , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.1 ; p [ 1 ] = 0.1 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: hip_right , p )); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.1 ; p [ 1 ] = 0.2 ; p [ 2 ] = 0.0 ; unordered . push_back ( std :: make_pair ( KeyPointTag :: knee_right , p )); } skeleton . setTag ( \"unordered\" ); skeleton . update ( unordered ); skeleton . print (); return EXIT_SUCCESS ; } The result is: tag = \"unordered\" transformation = -1.000 0.000 0.000 0.000 0.000 1.000 0.000 0.000 -0.000 0.000 -1.000 0.000 0.000 0.000 0.000 1.000 coronal = (-0.000 0.000 1.000) sagittal = (-1.000 0.000 -0.000) transverse = ( 0.000 -1.000 0.000) keypoint[\"shoulderCenter\"] = ( 0.000 0.000 0.000); pixel=( nan nan); status=updated; parent={}; child={\"head\" \"shoulderLeft\" \"shoulderRight\" \"hipCenter\" } keypoint[\"head\"] = ( 0.000 -0.100 0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={} keypoint[\"shoulderLeft\"] = (-0.100 0.000 -0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={\"elbowLeft\" } keypoint[\"elbowLeft\"] = (-0.200 0.000 -0.000); pixel=( nan nan); status=updated; parent={\"shoulderLeft\" }; child={\"handLeft\" } keypoint[\"handLeft\"] = (-0.300 0.000 -0.000); pixel=( nan nan); status=updated; parent={\"elbowLeft\" }; child={} keypoint[\"shoulderRight\"] = ( 0.100 0.000 0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={\"elbowRight\" } keypoint[\"elbowRight\"] = ( 0.200 0.000 0.000); pixel=( nan nan); status=updated; parent={\"shoulderRight\" }; child={\"handRight\" } keypoint[\"handRight\"] = ( 0.300 0.000 0.000); pixel=( nan nan); status=updated; parent={\"elbowRight\" }; child={} keypoint[\"hipCenter\"] = ( 0.000 0.100 0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={\"hipLeft\" \"hipRight\" } keypoint[\"hipLeft\"] = (-0.100 0.100 -0.000); pixel=( nan nan); status=updated; parent={\"hipCenter\" }; child={\"kneeLeft\" } keypoint[\"kneeLeft\"] = (-0.100 0.200 -0.000); pixel=( nan nan); status=updated; parent={\"hipLeft\" }; child={\"ankleLeft\" } keypoint[\"ankleLeft\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"kneeLeft\" }; child={\"footLeft\" } keypoint[\"footLeft\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"ankleLeft\" }; child={} keypoint[\"hipRight\"] = ( 0.100 0.100 0.000); pixel=( nan nan); status=updated; parent={\"hipCenter\" }; child={\"kneeRight\" } keypoint[\"kneeRight\"] = ( 0.100 0.200 0.000); pixel=( nan nan); status=updated; parent={\"hipRight\" }; child={\"ankleRight\" } keypoint[\"ankleRight\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"kneeRight\" }; child={\"footRight\" } keypoint[\"footRight\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"ankleRight\" }; child={} Note Don't get worried about nan in the pixel fields: it's correct since we didn't set them up yet. Uninitialized points and/or pixels take nan values.","title":"From an unordered list of keypoints"},{"location":"create_new_skeleton/#from-an-ordered-list-of-keypoints","text":"The following code snippet creates a SkeletonStd object from an ordered list of keypoints: #include <yarp/sig/Vector.h> #include \"AssistiveRehab/skeleton.h\" int main () { assistive_rehab :: SkeletonStd skeleton ; std :: vector < yarp :: sig :: Vector > ordered ; { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.0 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.0 ; p [ 1 ] =- 0.1 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.1 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.2 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.3 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.1 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.2 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.3 ; p [ 1 ] = 0.0 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.1 ; p [ 1 ] = 0.1 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] = 0.1 ; p [ 1 ] = 0.2 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.1 ; p [ 1 ] = 0.1 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } { yarp :: sig :: Vector p ( 3 ); p [ 0 ] =- 0.1 ; p [ 1 ] = 0.2 ; p [ 2 ] = 0.0 ; ordered . push_back ( p ); } skeleton . setTag ( \"ordered\" ); skeleton . update ( ordered ); skeleton . print (); return EXIT_SUCCESS ; } The result is: tag = \"ordered\" transformation = -1.000 0.000 0.000 0.000 0.000 1.000 0.000 0.000 -0.000 0.000 -1.000 0.000 0.000 0.000 0.000 1.000 coronal = (-0.000 0.000 0.707) sagittal = (-1.000 0.000 -0.000) transverse = ( 0.707 -0.707 0.000) keypoint[\"shoulderCenter\"] = ( 0.000 0.000 0.000); pixel=( nan nan); status=updated; parent={}; child={\"head\" \"shoulderLeft\" \"shoulderRight\" \"hipCenter\" } keypoint[\"head\"] = ( 0.000 -0.100 0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={} keypoint[\"shoulderLeft\"] = (-0.100 0.000 -0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={\"elbowLeft\" } keypoint[\"elbowLeft\"] = (-0.200 0.000 -0.000); pixel=( nan nan); status=updated; parent={\"shoulderLeft\" }; child={\"handLeft\" } keypoint[\"handLeft\"] = (-0.300 0.000 -0.000); pixel=( nan nan); status=updated; parent={\"elbowLeft\" }; child={} keypoint[\"shoulderRight\"] = ( 0.100 0.000 0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={\"elbowRight\" } keypoint[\"elbowRight\"] = ( 0.200 0.000 0.000); pixel=( nan nan); status=updated; parent={\"shoulderRight\" }; child={\"handRight\" } keypoint[\"handRight\"] = ( 0.300 0.000 0.000); pixel=( nan nan); status=updated; parent={\"elbowRight\" }; child={} keypoint[\"hipCenter\"] = (-0.100 0.100 -0.000); pixel=( nan nan); status=updated; parent={\"shoulderCenter\" }; child={\"hipLeft\" \"hipRight\" } keypoint[\"hipLeft\"] = (-0.100 0.200 -0.000); pixel=( nan nan); status=updated; parent={\"hipCenter\" }; child={\"kneeLeft\" } keypoint[\"kneeLeft\"] = ( 0.100 0.100 0.000); pixel=( nan nan); status=updated; parent={\"hipLeft\" }; child={\"ankleLeft\" } keypoint[\"ankleLeft\"] = ( 0.100 0.200 0.000); pixel=( nan nan); status=updated; parent={\"kneeLeft\" }; child={\"footLeft\" } keypoint[\"footLeft\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"ankleLeft\" }; child={} keypoint[\"hipRight\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"hipCenter\" }; child={\"kneeRight\" } keypoint[\"kneeRight\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"hipRight\" }; child={\"ankleRight\" } keypoint[\"ankleRight\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"kneeRight\" }; child={\"footRight\" } keypoint[\"footRight\"] = ( nan nan nan); pixel=( nan nan); status=stale; parent={\"ankleRight\" }; child={}","title":"From an ordered list of keypoints"},{"location":"create_new_skeleton/#from-a-property-like-structure","text":"Assuming that the object skeleton1 has been previously defined using one of the instructions above, a new Skeleton object can be defined as following: yarp :: os :: Property prop = skeleton1 . toProperty (); std :: unique_ptr < assistive_rehab :: Skeleton > skeleton2 ( assistive_rehab :: skeleton_factory ( prop )); skeleton2 -> setTag ( \"properties\" ); skeleton2 -> print ();","title":"From a property-like structure"},{"location":"create_new_skeleton/#retrieving-a-skeleton","text":"A skeleton can be retrieved as a property-like structure from a yarp port: assistive_rehab :: SkeletonStd skeleton ; if ( yarp :: os :: Bottle * b = opcPort . read ( false )) { yarp :: os :: Property prop ; prop . fromString ( b -> get ( 0 ). asList () -> toString ()); skeleton . update ( assistive_rehab :: skeleton_factory ( prop ) -> toProperty ()); } or from the OPC as following: assistive_rehab :: SkeletonStd skeleton ; yarp :: os :: Bottle cmd , reply ; cmd . addVocab ( yarp :: os :: Vocab :: encode ( \"ask\" )); yarp :: os :: Bottle & content = cmd . addList (). addList (); content . addString ( \"skeleton\" ); opcPort . write ( cmd , reply ); if ( reply . size () > 1 ) { if ( reply . get ( 0 ). asVocab () == Vocab :: encode ( \"ack\" )) { if ( yarp :: os :: Bottle * idField = reply . get ( 1 ). asList ()) { if ( yarp :: os :: Bottle * idValues = idField -> get ( 1 ). asList ()) { int id = idValues -> get ( 0 ). asInt (); cmd . clear (); cmd . addVocab ( Vocab :: encode ( \"get\" )); yarp :: os :: Bottle & content = cmd . addList (). addList (); yarp :: os :: Bottle replyProp ; content . addString ( \"id\" ); content . addInt ( id ); opcPort . write ( cmd , replyProp ); if ( replyProp . get ( 0 ). asVocab () == yarp :: os :: Vocab :: encode ( \"ack\" )) { if ( yarp :: os :: Bottle * propField = replyProp . get ( 1 ). asList ()) { yarp :: os :: Property prop ( propField -> toString (). c_str ()); skeleton . update ( assistive_rehab :: skeleton_factory ( prop ) -> toProperty ()); } } } } } }","title":"Retrieving a skeleton"},{"location":"create_new_skeleton/#accessing-single-keypoints-of-a-skeleton","text":"If you want to access to a keypoint of the skeleton, you can use the operator[] of the class Skeleton , by passing as parameter the keypoint's tag. For example, the following snippet allows you to get the shoulder_center 3D coordinates: yarp :: sig :: Vector sc = skeleton [ assistive_rehab :: KeyPointTag :: shoulder_center ] -> getPoint ();","title":"Accessing single keypoints of a skeleton"},{"location":"create_new_skeleton/#dealing-with-pixels","text":"Occasionally, it might be worth storing also the pixels alongside the points, which are used by the algorithm to reconstruct the 3D skeleton. This is particularly useful when the skeleton is employed to enable gaze tracking, for example. In this context, the 3D information of the keypoints needs normally to be transformed from the camera frame to the root frame of the robot. Instead, having the pixels would ease this process. The updating methods described above do have their pixel-wise counterparts: std :: vector < std :: pair < yarp :: sig :: Vector , yarp :: sig :: Vector >> ordered_withpixels ; yarp :: sig :: Vector p ( 3 , 0.1 ); yarp :: sig :: Vector px ( 2 , 10.0 ); ordered_withpixels . push_back ( std :: make_pair ( p , px )); skeleton . update_withpixels ( ordered_withpixels ); std :: vector < std :: pair < std :: string < std :: pair < yarp :: sig :: Vector , yarp :: sig :: Vector >>> unordered_withpixels ; yarp :: sig :: Vector p ( 3 , 0.1 ); yarp :: sig :: Vector px ( 2 , 10.0 ); unordered_withpixels . push_back ( std :: make_pair ( assistive_rehab :: KeyPointTag :: shoulder_center , std :: make_pair ( p , px ))); skeleton . update_withpixels ( unordered_withpixels ); yarp :: sig :: Vector pixel = skeleton [ assistive_rehab :: KeyPointTag :: shoulder_center ]. getPixel ();","title":"Dealing with pixels"},{"location":"create_new_skeleton/#additional-functionalities","text":"","title":"Additional functionalities"},{"location":"create_new_skeleton/#normalization","text":"Some applications might require a normalized skeleton, to avoid having different results for different human physiques. The normalization provided in the library makes the length of the observed human segments always equal to 1 and can be applied as following: skeleton . normalize ();","title":"Normalization"},{"location":"create_new_skeleton/#keypoints-reference-system","text":"Keypoints in the skeleton are defined with respect to the camera. To change the reference system, given a transformation matrix T , you can use the method setTransformation , as following: skeleton . setTransformation ( T ); Tip If you want to use the skeleton as reference system, you can create the transformation matrix T from the skeleton's planes: yarp :: sig :: Vector coronal = skeleton . getCoronal (); yarp :: sig :: Vector sagittal = skeleton . getSagittal (); yarp :: sig :: Vector transverse = skeleton . getTransverse (); yarp :: sig :: Vector p = skeleton [ assistive_rehab :: KeyPointTag :: shoulder_center ] -> getPoint (); yarp :: sig :: Matrix T1 ( 4 , 4 ); T1 . setSubcol ( coronal , 0 , 0 ); T1 . setSubcol ( sagittal , 0 , 1 ); T1 . setSubcol ( transverse , 0 , 2 ); T1 . setSubcol ( p , 0 , 3 ); T1 ( 3 , 3 ) = 1.0 ; T = yarp :: math :: SE3inv ( T1 ); skeleton . setTransformation ( T );","title":"Keypoints' reference system"},{"location":"disembodied/","text":"How to run the visual pipeline in a disembodied manner \u00b6 The visual pipeline can be run also without a physical robot, i.e. with a (disembodied) realsense camera . This tutorial will show you how to do it. Warning The visual pipeline relies on yarpOpenPose and actionRecognizer , which require an NVIDIA graphics card to be run. First, you need to run yarpserver . You can open a terminal and type: yarpserver Connect the realsense to your laptop. Seealso If you have not done it, you will need to install it. Warning yarp has to be compiled with ENABLE_yarpmod_realsense2 ON . Open yarpmanager , run the Assistive_Rehab_App and hit connect . Note All the modules that require a robot must not be run, i.e. faceDisplayServer , faceExpressionImage , attentionManager , cer_gaze-controller , interactionManager , ctpService , cer_reaching-solver and cer_reaching-controller . Tip You can customize the app by removing unnecessary modules and replacing the nodes in the xml with localhost. You can save the app in the folder $HOME/.local/share/yarp and exploit the shadowing mechanism . In this way, when you open yarpmanager , the app will be automatically loaded. Once all the modules are running, you need to send commands to motionAnalyzer , in order to select the metric and the skeleton that you want to analyze. Therefore you can open a terminal and type: yarp rpc /motionAnalyzer/cmd loadMetric metric_tag selectSkel skeleton_tag start where metric_tag and skeleton_tag are respectively the tag of the metric and the tag of the skeleton under analysis (for example ROM_0 and #0 ). Tip The available metrics can be listed using the command listMetrics . When the command start is given to motionAnalyzer , the visual pipeline starts, i.e. the template skeleton is loaded and visualized on skeletonViewer , the extracted metric is visualized on yarpscope and the action recognition pipeline starts. You should have the following situation: A verbal feedback is also provided throughout the experiment. When you want to stop the pipeline, you need to send a stop command to motionAnalyzer , by typing stop in the previous terminal.","title":"How to run the pipeline in a disembodied manner"},{"location":"disembodied/#how-to-run-the-visual-pipeline-in-a-disembodied-manner","text":"The visual pipeline can be run also without a physical robot, i.e. with a (disembodied) realsense camera . This tutorial will show you how to do it. Warning The visual pipeline relies on yarpOpenPose and actionRecognizer , which require an NVIDIA graphics card to be run. First, you need to run yarpserver . You can open a terminal and type: yarpserver Connect the realsense to your laptop. Seealso If you have not done it, you will need to install it. Warning yarp has to be compiled with ENABLE_yarpmod_realsense2 ON . Open yarpmanager , run the Assistive_Rehab_App and hit connect . Note All the modules that require a robot must not be run, i.e. faceDisplayServer , faceExpressionImage , attentionManager , cer_gaze-controller , interactionManager , ctpService , cer_reaching-solver and cer_reaching-controller . Tip You can customize the app by removing unnecessary modules and replacing the nodes in the xml with localhost. You can save the app in the folder $HOME/.local/share/yarp and exploit the shadowing mechanism . In this way, when you open yarpmanager , the app will be automatically loaded. Once all the modules are running, you need to send commands to motionAnalyzer , in order to select the metric and the skeleton that you want to analyze. Therefore you can open a terminal and type: yarp rpc /motionAnalyzer/cmd loadMetric metric_tag selectSkel skeleton_tag start where metric_tag and skeleton_tag are respectively the tag of the metric and the tag of the skeleton under analysis (for example ROM_0 and #0 ). Tip The available metrics can be listed using the command listMetrics . When the command start is given to motionAnalyzer , the visual pipeline starts, i.e. the template skeleton is loaded and visualized on skeletonViewer , the extracted metric is visualized on yarpscope and the action recognition pipeline starts. You should have the following situation: A verbal feedback is also provided throughout the experiment. When you want to stop the pipeline, you need to send a stop command to motionAnalyzer , by typing stop in the previous terminal.","title":"How to run the visual pipeline in a disembodied manner"},{"location":"install/","text":"Install \u00b6 Disclaimer Assistive-rehab has been widely tested on Ubuntu 16.04 and Ubuntu 18.04 . If you face any issue either with your OS, please submit an Issue . Requirements \u00b6 Supported Operating Systems: Linux, Windows, macOS C++11 compiler CMake 3.5 icub-contrib-common yarp (3.1.100 or higher) iCub OpenCV (3.4.0 or higher) yarpOpenPose Ipopt yarp ENABLE_yarpcar_mjpeg ON : to allow mjpeg compression. ENABLE_yarpcar_zfp ON : to allow zfp compression. ENABLE_yarpmod_realsense2 ON : to enable the realsense. OpenCV Download OpenCV: git clone https://github.com/opencv/opencv.git . Checkout the correct branch/tag: git checkout 3.4.0 . Download the external modules: git clone https://github.com/opencv/opencv_contrib.git . Checkout the same branch/tag: git checkout 3.4.0 . Configure OpenCV by filling in OPENCV_EXTRA_MODULES_PATH with the path to opencv_contrib/modules and then toggling on all possible modules. Compile OpenCV. Optional dependencies \u00b6 Dependency License TensorFlowCC MIT fftw3 GPL GSL GPL matio BSD 2-Clause VTK (8.1.0 or higher) BSD-style cer GPL TensorFlowCC TensorFlowCC builds and installs the TensorFlow C++ API, which is released under Apache 2.0 license. matio On Ubuntu 18.04 , you can install the library through apt: sudo apt install libmatio-dev . Warning If an optional dependency is not found, the modules depending on it are not compiled. Failure If you want to run the full demo, also additional dependencies are required. report For generating the offline report, you will need to install the following python libraries (you can install them through pip install ): scipy numpy matplotlib pandas glob jupyter plotly warnings collections math re os datetime IPython plotly You need to enable jupyter extension to allow plotly to work in jupyter notebook: pip install \"notebook>=5.3\" \"ipywidgets>=7.2\" --user . Installation \u00b6 If all the dependencies are met, proceed with the following instructions: From sources Substitute to <install-prefix> the absolute path where you want to install the project. GNU/Linux and macOS git clone https://github.com/robotology/assistive-rehab.git mkdir build && cd build cmake .. -DCMAKE_INSTALL_PREFIX=<install-prefix> make make install Windows git clone https://github.com/robotology/assistive-rehab.git mkdir build && cd build cmake .. -DCMAKE_INSTALL_PREFIX=<install-prefix> make make install","title":"Install"},{"location":"install/#install","text":"Disclaimer Assistive-rehab has been widely tested on Ubuntu 16.04 and Ubuntu 18.04 . If you face any issue either with your OS, please submit an Issue .","title":"Install"},{"location":"install/#requirements","text":"Supported Operating Systems: Linux, Windows, macOS C++11 compiler CMake 3.5 icub-contrib-common yarp (3.1.100 or higher) iCub OpenCV (3.4.0 or higher) yarpOpenPose Ipopt yarp ENABLE_yarpcar_mjpeg ON : to allow mjpeg compression. ENABLE_yarpcar_zfp ON : to allow zfp compression. ENABLE_yarpmod_realsense2 ON : to enable the realsense. OpenCV Download OpenCV: git clone https://github.com/opencv/opencv.git . Checkout the correct branch/tag: git checkout 3.4.0 . Download the external modules: git clone https://github.com/opencv/opencv_contrib.git . Checkout the same branch/tag: git checkout 3.4.0 . Configure OpenCV by filling in OPENCV_EXTRA_MODULES_PATH with the path to opencv_contrib/modules and then toggling on all possible modules. Compile OpenCV.","title":"Requirements"},{"location":"install/#optional-dependencies","text":"Dependency License TensorFlowCC MIT fftw3 GPL GSL GPL matio BSD 2-Clause VTK (8.1.0 or higher) BSD-style cer GPL TensorFlowCC TensorFlowCC builds and installs the TensorFlow C++ API, which is released under Apache 2.0 license. matio On Ubuntu 18.04 , you can install the library through apt: sudo apt install libmatio-dev . Warning If an optional dependency is not found, the modules depending on it are not compiled. Failure If you want to run the full demo, also additional dependencies are required. report For generating the offline report, you will need to install the following python libraries (you can install them through pip install ): scipy numpy matplotlib pandas glob jupyter plotly warnings collections math re os datetime IPython plotly You need to enable jupyter extension to allow plotly to work in jupyter notebook: pip install \"notebook>=5.3\" \"ipywidgets>=7.2\" --user .","title":"Optional dependencies"},{"location":"install/#installation","text":"If all the dependencies are met, proceed with the following instructions: From sources Substitute to <install-prefix> the absolute path where you want to install the project. GNU/Linux and macOS git clone https://github.com/robotology/assistive-rehab.git mkdir build && cd build cmake .. -DCMAKE_INSTALL_PREFIX=<install-prefix> make make install Windows git clone https://github.com/robotology/assistive-rehab.git mkdir build && cd build cmake .. -DCMAKE_INSTALL_PREFIX=<install-prefix> make make install","title":"Installation"},{"location":"latest_news/","text":"Latest news \u00b6 July 10, 2019 : Checkout our latest release v0.4.0 ! What's new? the feedback can be provided now using the robot skeleton template , rather than the pre-recorded one. The new module robotSkeletonPublisher publishes the robot skeleton, which represents R1 limbs configuration, as following: The robot skeleton is remapped onto the observed skeleton internally within feedbackProducer for the further analysis ( skeletonScaler and skeletonPlayer are thus bypassed). Such modality insures a full synchronization between the robot movement and the skeleton template, which was not guaranteed with the pre-recorded template. Note The modality with the pre-recorded template is still available and can be set through interactionManager by setting the flag use-robot-template to false . In such case, the pipeline including skeletonScaler and skeletonPlayer is used. Tip The robot skeleton can be replayed offline by saving the robot joints specified in this app . A tutorial for replaying a full experiment can be found in the Tutorial section. the Train With Me study aims at comparing users' engagement during a physical training session with a real robot and a virtual agent. Preliminary experiments were designed for comparing R1 with its virtual counterpart and the developed infrastructure is now available. interactionManager can deal with the following three phases: observation : the real/virtual robot shows the exercise and the user observes it; imitation : the real/virtual robot performs the exercise and the user imitates it, occlusion : the real/virtual robot keeps performing the exercise behind a panel and the user keeps imitating it, without having any feedback. The scripts used during experiments can be found here , namely AssistiveRehab-TWM-robot.xml.template and AssistiveRehab-TWM-virtual.xml.template , which load parameters defined in the train-with-me context. A tutorial for running the demo with the virtual R1 can be found in the Tutorial section. May 6, 2019 : Checkout our latest release v0.3.0 ! This is a major change which refactors the entire framework to deal also with feet, following up the use of BODY_25 model of OpenPose . The following is an example of skeleton with feet in 2D and 3D: Changes include: SkeletonStd now includes hip_center , foot_left , foot_right : hip_center is directly observed if available, otherwise is estimated as middle point between hip_left and hip_right (the same stands for shoulder_center ); foot_left and foot_right are detected as being the big-toe. If big-toe is not available, small-toe is used as fallback; SkeletonWaist has been removed in favor of the new SkeletonStd ; optimization performed by skeletonRetriever is now extended also to lower limbs; modules previously relying on SkeletonWaist have been updated to use the new SkeletonStd ; the new framework is compatible with the Y1M5 demo , which was successfully tested online on the robot; the new framework is compatible with datasets recorded before the release, which can be reproduced by means of yarpdataplayer . May 6, 2019 : Checkout our new release v0.2.1 ! What's new? the action recognition is now robust to rotations ! The original network was trained with skeletons frontal to the camera, which is not guaranteed during a real interaction. The network has been re-trained with a wider training set, comprising synthetic rotations applied to real data around each axis, with variation of 10 degrees in a range of [-20,20] degrees. Also a variability on the speed was introduced in the training set, by considering the action performed at normal, double and half speed. We compared the accuracy of the previous and the new model for different rotations of the skeleton, and results show a high accuracy to a wider range for all axes: ROTATION AROUND X Original New ROTATION AROUND Y Original New ROTATION AROUND Z Original New the skeleton now also stores the pixels alongside the 3D points! This is very useful when using the skeleton for gaze tracking, as it avoids the transformation from the camera frame to the root frame of the robot required is using 3D information; the offline report is now interactive ! The user can navigate through plots, zoom, pan, save: January 28, 2019 : Checkout our latest tutorial on how to run the main applications on the robot R1! January 25, 2019 : We are public now! December 21, 2018 : Checkout the latest release and the the comparison with the first release !","title":"Latest"},{"location":"latest_news/#latest-news","text":"July 10, 2019 : Checkout our latest release v0.4.0 ! What's new? the feedback can be provided now using the robot skeleton template , rather than the pre-recorded one. The new module robotSkeletonPublisher publishes the robot skeleton, which represents R1 limbs configuration, as following: The robot skeleton is remapped onto the observed skeleton internally within feedbackProducer for the further analysis ( skeletonScaler and skeletonPlayer are thus bypassed). Such modality insures a full synchronization between the robot movement and the skeleton template, which was not guaranteed with the pre-recorded template. Note The modality with the pre-recorded template is still available and can be set through interactionManager by setting the flag use-robot-template to false . In such case, the pipeline including skeletonScaler and skeletonPlayer is used. Tip The robot skeleton can be replayed offline by saving the robot joints specified in this app . A tutorial for replaying a full experiment can be found in the Tutorial section. the Train With Me study aims at comparing users' engagement during a physical training session with a real robot and a virtual agent. Preliminary experiments were designed for comparing R1 with its virtual counterpart and the developed infrastructure is now available. interactionManager can deal with the following three phases: observation : the real/virtual robot shows the exercise and the user observes it; imitation : the real/virtual robot performs the exercise and the user imitates it, occlusion : the real/virtual robot keeps performing the exercise behind a panel and the user keeps imitating it, without having any feedback. The scripts used during experiments can be found here , namely AssistiveRehab-TWM-robot.xml.template and AssistiveRehab-TWM-virtual.xml.template , which load parameters defined in the train-with-me context. A tutorial for running the demo with the virtual R1 can be found in the Tutorial section. May 6, 2019 : Checkout our latest release v0.3.0 ! This is a major change which refactors the entire framework to deal also with feet, following up the use of BODY_25 model of OpenPose . The following is an example of skeleton with feet in 2D and 3D: Changes include: SkeletonStd now includes hip_center , foot_left , foot_right : hip_center is directly observed if available, otherwise is estimated as middle point between hip_left and hip_right (the same stands for shoulder_center ); foot_left and foot_right are detected as being the big-toe. If big-toe is not available, small-toe is used as fallback; SkeletonWaist has been removed in favor of the new SkeletonStd ; optimization performed by skeletonRetriever is now extended also to lower limbs; modules previously relying on SkeletonWaist have been updated to use the new SkeletonStd ; the new framework is compatible with the Y1M5 demo , which was successfully tested online on the robot; the new framework is compatible with datasets recorded before the release, which can be reproduced by means of yarpdataplayer . May 6, 2019 : Checkout our new release v0.2.1 ! What's new? the action recognition is now robust to rotations ! The original network was trained with skeletons frontal to the camera, which is not guaranteed during a real interaction. The network has been re-trained with a wider training set, comprising synthetic rotations applied to real data around each axis, with variation of 10 degrees in a range of [-20,20] degrees. Also a variability on the speed was introduced in the training set, by considering the action performed at normal, double and half speed. We compared the accuracy of the previous and the new model for different rotations of the skeleton, and results show a high accuracy to a wider range for all axes: ROTATION AROUND X Original New ROTATION AROUND Y Original New ROTATION AROUND Z Original New the skeleton now also stores the pixels alongside the 3D points! This is very useful when using the skeleton for gaze tracking, as it avoids the transformation from the camera frame to the root frame of the robot required is using 3D information; the offline report is now interactive ! The user can navigate through plots, zoom, pan, save: January 28, 2019 : Checkout our latest tutorial on how to run the main applications on the robot R1! January 25, 2019 : We are public now! December 21, 2018 : Checkout the latest release and the the comparison with the first release !","title":"Latest news"},{"location":"license/","text":"BSD 3-Clause \u00b6 BSD 3-Clause License Copyright \u00a9 2018, Robotology All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"License"},{"location":"license/#bsd-3-clause","text":"BSD 3-Clause License Copyright \u00a9 2018, Robotology All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"BSD 3-Clause"},{"location":"main_apps/","text":"How to run the main applications \u00b6 This tutorial will show you how to run the main applications of the repository. The main applications can be found here , namely AssistiveRehab.xml.template and AssistiveRehab-faces.xml.template . The basic app: AssistiveRehab.xml \u00b6 Let's start with the basic one: AssistiveRehab.xml.template . Tip AssistiveRehab-faces.xml.template builds upon AssistiveRehab.xml.template , introducing additional modules for face recognition. We assume you are working on the robot R1 and that r1-face , r1-torso1 , r1-cuda-linux , r1-console-linux , r1-base and r1-display-linux are available. R1 On R1, yarpserver runs on r1-base . R1-mk2 On R1-mk2, the cuda system is named r1-console-cuda , which we also use as display (therefore r1-display-linux is r1-console-cuda ). The interaction requires the robot's motors to be on. Therefore turn on the motors, then open a terminal and type: ssh r1-base cd $ROBOT_CODE/cer/app/robots/CER02 yarprobotinterface Now we are ready to run the application! Open a new terminal and type: yarpmanager Now click on Assistive_Rehab_App , hit run and then connect. The demo is now running! To make the robot look around and engage the user, just issue the start command to the interactionManager module. If the user accepts the invitation by lifting her/his hand, the interaction starts. The user has to repeat the exercise shown by the robot, which in turns evaluates how the exercise is being performed, through a verbal feedback. The session ends with the robot giving an overall feedback of the exercise. The interaction can go on, as the robot keeps memory of the past interaction. Let's look a bit deeper into the application to see which are the modules running: yarpdev --device speech --lingware-context speech --default-language it-IT --robot r1 --pitch 80 --speed 110 : to run the speech, by default in italian. You have to change the default-language to en-GB to switch to english; yarpdev --device faceDisplayServer : to run the face; yarpdev --device --context AssistiveRehab --from realsense2.ini : to run the camera; faceExpressionImage : to visualize the expressions and the talking mouth on the robot's display; iSpeak --package speech-dev : to make the robot speak; yarpOpenPose : to detect 2D skeletons; objectsPropertiesCollector : to store 3D skeletons within a yarp-oriented database; skeletonRetriever : to produce 3D skeletons from depth and 2D skeletons; attentionManager : to make the robot focus and follow a skeleton; cer_gaze-controller : to control the robot's gaze; motionAnalyzer --from motion-repertoire-rom12.ini : to analyze motion. This configuration file does not include the reaching exercise, which is under testing from the robot's point of view (the motion analysis is implemented); skeletonPlayer : to replay the template skeleton; skeletonScaler : to move/rescale the template skeleton; feedbackProducer : to produce a feedback depending on the exercise; actionRecognizer : to recognize the exercise being performed; feedbackSynthetizer : to generate a verbal feedback; yarpview : to visualize depth and RGB image with 2D skeletons; skeletonViewer : to visualize 3D skeletons; yarpscope : to visualize the metric on the movement in real-time; interactionManager : to supervise the interaction; ctpService (for each arm): to send commands to the robot's arms; cer_reaching-solver + cer_reaching-controller (for each arm): to let the robot performing a reaching task. When you want to stop the interaction, hit stop in yarpmanager . You can produce two final reports (in italian and english) of the performed exercises by opening a terminal and typing: assistive-rehab-generate-report.sh report For generating the offline report, you will need to install the following python libraries (you can install them through pip install ): scipy numpy matplotlib pandas glob jupyter plotly plotly You need to enable jupyter extension to allow plotly to work in jupyter notebook: pip install \"notebook>=5.3\" \"ipywidgets>=7.2\" --user . Two html files will be created in the folder where you run the script. Note Producing the report might take a while, as all the files whose names equal the name of the most recent file are processed. This allows us to create a report not only of the current session, but also of the clinical evolution of the patient. The basic app integrated with face recognition: AssistiveRehab-faces.xml \u00b6 The AssistiveRehab-faces.xml.template builds upon the basic app, introducing additional modules for face recognition, which are the following: R1-mk2 On R1-mk2, the face recognition pipeline is run on r1-torso2 , which has to be available. humanStructure : to restrict the area of recognition around human faces; caffeCoder : to extract a representation of the cropped image; linearClassifierModule : to classify the extracted features; faceRecognizer : to label human faces. For running the demo, click on Assistive_Rehab_with_Faces_App in yarpmanager , hit run and then connect. Note caffeCoder takes a while to load the network. Therefore, before hitting connect, wait some seconds. You can click the refresh icon and check when the caffeCoder ports become green. The demo is similar to the basic one, but now the robot interacts with people using their names! Tip If you want to train the network with your face to the face database, you can use the provided API .","title":"How to run the main applications"},{"location":"main_apps/#how-to-run-the-main-applications","text":"This tutorial will show you how to run the main applications of the repository. The main applications can be found here , namely AssistiveRehab.xml.template and AssistiveRehab-faces.xml.template .","title":"How to run the main applications"},{"location":"main_apps/#the-basic-app-assistiverehabxml","text":"Let's start with the basic one: AssistiveRehab.xml.template . Tip AssistiveRehab-faces.xml.template builds upon AssistiveRehab.xml.template , introducing additional modules for face recognition. We assume you are working on the robot R1 and that r1-face , r1-torso1 , r1-cuda-linux , r1-console-linux , r1-base and r1-display-linux are available. R1 On R1, yarpserver runs on r1-base . R1-mk2 On R1-mk2, the cuda system is named r1-console-cuda , which we also use as display (therefore r1-display-linux is r1-console-cuda ). The interaction requires the robot's motors to be on. Therefore turn on the motors, then open a terminal and type: ssh r1-base cd $ROBOT_CODE/cer/app/robots/CER02 yarprobotinterface Now we are ready to run the application! Open a new terminal and type: yarpmanager Now click on Assistive_Rehab_App , hit run and then connect. The demo is now running! To make the robot look around and engage the user, just issue the start command to the interactionManager module. If the user accepts the invitation by lifting her/his hand, the interaction starts. The user has to repeat the exercise shown by the robot, which in turns evaluates how the exercise is being performed, through a verbal feedback. The session ends with the robot giving an overall feedback of the exercise. The interaction can go on, as the robot keeps memory of the past interaction. Let's look a bit deeper into the application to see which are the modules running: yarpdev --device speech --lingware-context speech --default-language it-IT --robot r1 --pitch 80 --speed 110 : to run the speech, by default in italian. You have to change the default-language to en-GB to switch to english; yarpdev --device faceDisplayServer : to run the face; yarpdev --device --context AssistiveRehab --from realsense2.ini : to run the camera; faceExpressionImage : to visualize the expressions and the talking mouth on the robot's display; iSpeak --package speech-dev : to make the robot speak; yarpOpenPose : to detect 2D skeletons; objectsPropertiesCollector : to store 3D skeletons within a yarp-oriented database; skeletonRetriever : to produce 3D skeletons from depth and 2D skeletons; attentionManager : to make the robot focus and follow a skeleton; cer_gaze-controller : to control the robot's gaze; motionAnalyzer --from motion-repertoire-rom12.ini : to analyze motion. This configuration file does not include the reaching exercise, which is under testing from the robot's point of view (the motion analysis is implemented); skeletonPlayer : to replay the template skeleton; skeletonScaler : to move/rescale the template skeleton; feedbackProducer : to produce a feedback depending on the exercise; actionRecognizer : to recognize the exercise being performed; feedbackSynthetizer : to generate a verbal feedback; yarpview : to visualize depth and RGB image with 2D skeletons; skeletonViewer : to visualize 3D skeletons; yarpscope : to visualize the metric on the movement in real-time; interactionManager : to supervise the interaction; ctpService (for each arm): to send commands to the robot's arms; cer_reaching-solver + cer_reaching-controller (for each arm): to let the robot performing a reaching task. When you want to stop the interaction, hit stop in yarpmanager . You can produce two final reports (in italian and english) of the performed exercises by opening a terminal and typing: assistive-rehab-generate-report.sh report For generating the offline report, you will need to install the following python libraries (you can install them through pip install ): scipy numpy matplotlib pandas glob jupyter plotly plotly You need to enable jupyter extension to allow plotly to work in jupyter notebook: pip install \"notebook>=5.3\" \"ipywidgets>=7.2\" --user . Two html files will be created in the folder where you run the script. Note Producing the report might take a while, as all the files whose names equal the name of the most recent file are processed. This allows us to create a report not only of the current session, but also of the clinical evolution of the patient.","title":"The basic app: AssistiveRehab.xml"},{"location":"main_apps/#the-basic-app-integrated-with-face-recognition-assistiverehab-facesxml","text":"The AssistiveRehab-faces.xml.template builds upon the basic app, introducing additional modules for face recognition, which are the following: R1-mk2 On R1-mk2, the face recognition pipeline is run on r1-torso2 , which has to be available. humanStructure : to restrict the area of recognition around human faces; caffeCoder : to extract a representation of the cropped image; linearClassifierModule : to classify the extracted features; faceRecognizer : to label human faces. For running the demo, click on Assistive_Rehab_with_Faces_App in yarpmanager , hit run and then connect. Note caffeCoder takes a while to load the network. Therefore, before hitting connect, wait some seconds. You can click the refresh icon and check when the caffeCoder ports become green. The demo is similar to the basic one, but now the robot interacts with people using their names! Tip If you want to train the network with your face to the face database, you can use the provided API .","title":"The basic app integrated with face recognition: AssistiveRehab-faces.xml"},{"location":"replay_an_experiment/","text":"How to save and replay an experiment \u00b6 This tutorial will show you how to save and replay an experiment. Saving an experiment \u00b6 You can save an experiment by running this application , which runs several yarpdatadumper for saving: data from the camera and yarpOpenPose : 2D skeleton data, depth image and RGB image with 2D skeletons. robot joints. Saved data can be found in the folder skeletonDumper. Note You can save the data from the virtual robot by running this application . Replaying an experiment \u00b6 After saving an experiment as previously explained, this application allows you to replay it, i.e. to visualize the saved depth, the 2D skeletons, retrieve and visualize the 3D skeletons, publish and visualize the robot skeleton. Tip The application is conceived to visualize saved data. You can run any additional module that takes as input the saved data. It runs the following modules: objectsPropertiesCollector : to store 3D skeletons within a yarp-oriented database; skeletonRetriever : to produce 3D skeletons from depth and 2D skeletons; robotSkeletonPublisher : to produce 3D robot skeleton from the saved joints; skeletonViewer : to visualize 3D skeletons; yarpview : to visualize depth and RGB image with 2D skeletons. Tip If you are only interested in the visual pipeline from the camera, you can avoid running robotSkeletonPublisher . To run the application, you need a yarpserver . Open a terminal and type: yarpserver You need a yarpdataplayer to reproduce the dataset you have previously acquired. Open a terminal and type: yarpdataplayer An interface appears, you can click on File , then Open Directory . Click on the folder where you have saved your data (the folder should contain subfolders with 2D skeleton data, depth image and RGB image with 2D skeletons) and press play. Finally open yarpmanager , click on the Assistive_Rehab_Replay_App , hit run and then connect. Note robotSkeletonPublisher automatically connects to the robot ports. If you saved the data from the virtual robot using this app , you will need to specify the parameter --robot SIM_CER_ROBOT . Tip Replaying an experiment is an important feature, with the twofold objective of: allowing the physiotherapist to compute new metrics on the replayed experiment not run online; performing code debbuging.","title":"How to replay an experiment"},{"location":"replay_an_experiment/#how-to-save-and-replay-an-experiment","text":"This tutorial will show you how to save and replay an experiment.","title":"How to save and replay an experiment"},{"location":"replay_an_experiment/#saving-an-experiment","text":"You can save an experiment by running this application , which runs several yarpdatadumper for saving: data from the camera and yarpOpenPose : 2D skeleton data, depth image and RGB image with 2D skeletons. robot joints. Saved data can be found in the folder skeletonDumper. Note You can save the data from the virtual robot by running this application .","title":"Saving an experiment"},{"location":"replay_an_experiment/#replaying-an-experiment","text":"After saving an experiment as previously explained, this application allows you to replay it, i.e. to visualize the saved depth, the 2D skeletons, retrieve and visualize the 3D skeletons, publish and visualize the robot skeleton. Tip The application is conceived to visualize saved data. You can run any additional module that takes as input the saved data. It runs the following modules: objectsPropertiesCollector : to store 3D skeletons within a yarp-oriented database; skeletonRetriever : to produce 3D skeletons from depth and 2D skeletons; robotSkeletonPublisher : to produce 3D robot skeleton from the saved joints; skeletonViewer : to visualize 3D skeletons; yarpview : to visualize depth and RGB image with 2D skeletons. Tip If you are only interested in the visual pipeline from the camera, you can avoid running robotSkeletonPublisher . To run the application, you need a yarpserver . Open a terminal and type: yarpserver You need a yarpdataplayer to reproduce the dataset you have previously acquired. Open a terminal and type: yarpdataplayer An interface appears, you can click on File , then Open Directory . Click on the folder where you have saved your data (the folder should contain subfolders with 2D skeleton data, depth image and RGB image with 2D skeletons) and press play. Finally open yarpmanager , click on the Assistive_Rehab_Replay_App , hit run and then connect. Note robotSkeletonPublisher automatically connects to the robot ports. If you saved the data from the virtual robot using this app , you will need to specify the parameter --robot SIM_CER_ROBOT . Tip Replaying an experiment is an important feature, with the twofold objective of: allowing the physiotherapist to compute new metrics on the replayed experiment not run online; performing code debbuging.","title":"Replaying an experiment"},{"location":"tutorial_intro/","text":"Getting started \u00b6 If you want to learn how to exploit our framework to create something similar for your own applications, you can start from here: How to manage a skeleton object How to replay an experiment How to run the visual pipeline in a disembodied manner How to temporally align two signals How to run the main applications How to run the virtual demo","title":"Getting started"},{"location":"tutorial_intro/#getting-started","text":"If you want to learn how to exploit our framework to create something similar for your own applications, you can start from here: How to manage a skeleton object How to replay an experiment How to run the visual pipeline in a disembodied manner How to temporally align two signals How to run the main applications How to run the virtual demo","title":"Getting started"},{"location":"virtual_demo/","text":"How to run the virtual demo \u00b6 This tutorial will show you how to run the virtual demo. The virtual demo replicates the demo with the real R1 in a virtual environment ( Gazebo ), with the virtual version of the robot: The virtual R1 is shown on a screen with a RealSense on the top (indicated by the red arrow in the picture). In this demo, the user in the field of view is automatically engaged and the interaction includes three phases: observation phase : the virtual robot welcomes the user and shows the exercise to perform; direct imitation phase : the virtual robot performs the exercise together with the user, while providing a verbal feedback on how the exercise is being performed; occluded imitation phase : the virtual robot keeps performing the exercise behind a panel and stops providing the verbal feedback . Features like facial expressions, gazing the user and a verbal feedback are also included, such that the interaction is as close as possible to the real one. The related application can be found here , named AssistiveRehab-TWM-virtual.xml.template . Dependencies \u00b6 After installing assistive-rehab , you will need the following dependencies: RealSense : for running the RealSense; cer : for running the gaze-controller and face expressions; gazebo : for running the virtual environment; gazebo-yarp-plugins : for exposing YARP interfaces in Gazebo ; cer-sim : which includes the model loaded by Gazebo in AssistiveRehab-TWM-virtual.xml.template ; speech : for running the iSpeak module. Requirements \u00b6 The following hardware is required: RealSense camera ; NVIDIA graphics card : for running yarpOpenPose and actionRecognizer . Run the virtual demo \u00b6 To run the demo, first run yarpserver . Connect the RealSense to your laptop. Open yarpmanager , run the AssistiveRehab-TWM-virtual App and connect. A virtual R1 appears within the simulation environment. Note By default, Gazebo has the origin and the external gui visible. To remove the origin, you can click on View and deselect Origin . To remove the gui, you can click on Window and Full screen . When the demo is launched, the interactionManager waits for the command start_observation to start the exercise session. Specifically, the following commands should be sent in this order to the rpc port /interactionManager/cmd:rpc : start_observation : to start the observation phase, where the robot looks for a user and, when it founds her/him, starts showing the exercise to be performed; start_imitation : to start the direct imitation phase, where the robot performs the exercise together with the user, while providing her/him with verbal feedback; start_occlusion : to start the occluded imitation phase, where the robot keeps performing the exercise behind a panel and stops providing the verbal feedback; stop : to stop the interaction. Tip A different kind of interaction is also allowed, requiring the user to raise her/his hand to start the interaction. Such interaction includes the observation and the direct imitation phases, with the virtual robot showing the exercise to perform and, right after, performing the exercise while providing a verbal feedback (this is the virtual version of the demo Y1M5 ). In this configuration, the command start_with_hand starts the interaction and, when the user raises her/his hand, the two phases are run in a row, one after the other, without waiting any additional command. The following picture shows an example of interaction during the direct and occluded imitation phase: During the direct imitation phase, the robot moves while providing verbal feedback (the mouth appears on the robot's screen). During the occluded imitation phase, a panel appears occluding the moving arm and the robot stops providing verbal feedback (the mouth only appears at the end of the exercise, when the robot warns the user that the exercise is over). Note The shown interaction is just an example and the number of repetitions of the exercise is higher by default. The parameters used for this application can be found in the context train-with-me . Specifically, parameters that control the repetitions of the arm movements are defined in the interactionManager.ini as nrep-show and nrep-perform , respectively for the observation phase (set to 8 by default) and the imitation phase, both direct and occluded (set to 16 by default). The command start_occlusion can be sent after the eighth repetition of the movement within the direct phase, to have 8 repetitions for each phase.","title":"How to run the virtual demo"},{"location":"virtual_demo/#how-to-run-the-virtual-demo","text":"This tutorial will show you how to run the virtual demo. The virtual demo replicates the demo with the real R1 in a virtual environment ( Gazebo ), with the virtual version of the robot: The virtual R1 is shown on a screen with a RealSense on the top (indicated by the red arrow in the picture). In this demo, the user in the field of view is automatically engaged and the interaction includes three phases: observation phase : the virtual robot welcomes the user and shows the exercise to perform; direct imitation phase : the virtual robot performs the exercise together with the user, while providing a verbal feedback on how the exercise is being performed; occluded imitation phase : the virtual robot keeps performing the exercise behind a panel and stops providing the verbal feedback . Features like facial expressions, gazing the user and a verbal feedback are also included, such that the interaction is as close as possible to the real one. The related application can be found here , named AssistiveRehab-TWM-virtual.xml.template .","title":"How to run the virtual demo"},{"location":"virtual_demo/#dependencies","text":"After installing assistive-rehab , you will need the following dependencies: RealSense : for running the RealSense; cer : for running the gaze-controller and face expressions; gazebo : for running the virtual environment; gazebo-yarp-plugins : for exposing YARP interfaces in Gazebo ; cer-sim : which includes the model loaded by Gazebo in AssistiveRehab-TWM-virtual.xml.template ; speech : for running the iSpeak module.","title":"Dependencies"},{"location":"virtual_demo/#requirements","text":"The following hardware is required: RealSense camera ; NVIDIA graphics card : for running yarpOpenPose and actionRecognizer .","title":"Requirements"},{"location":"virtual_demo/#run-the-virtual-demo","text":"To run the demo, first run yarpserver . Connect the RealSense to your laptop. Open yarpmanager , run the AssistiveRehab-TWM-virtual App and connect. A virtual R1 appears within the simulation environment. Note By default, Gazebo has the origin and the external gui visible. To remove the origin, you can click on View and deselect Origin . To remove the gui, you can click on Window and Full screen . When the demo is launched, the interactionManager waits for the command start_observation to start the exercise session. Specifically, the following commands should be sent in this order to the rpc port /interactionManager/cmd:rpc : start_observation : to start the observation phase, where the robot looks for a user and, when it founds her/him, starts showing the exercise to be performed; start_imitation : to start the direct imitation phase, where the robot performs the exercise together with the user, while providing her/him with verbal feedback; start_occlusion : to start the occluded imitation phase, where the robot keeps performing the exercise behind a panel and stops providing the verbal feedback; stop : to stop the interaction. Tip A different kind of interaction is also allowed, requiring the user to raise her/his hand to start the interaction. Such interaction includes the observation and the direct imitation phases, with the virtual robot showing the exercise to perform and, right after, performing the exercise while providing a verbal feedback (this is the virtual version of the demo Y1M5 ). In this configuration, the command start_with_hand starts the interaction and, when the user raises her/his hand, the two phases are run in a row, one after the other, without waiting any additional command. The following picture shows an example of interaction during the direct and occluded imitation phase: During the direct imitation phase, the robot moves while providing verbal feedback (the mouth appears on the robot's screen). During the occluded imitation phase, a panel appears occluding the moving arm and the robot stops providing verbal feedback (the mouth only appears at the end of the exercise, when the robot warns the user that the exercise is over). Note The shown interaction is just an example and the number of repetitions of the exercise is higher by default. The parameters used for this application can be found in the context train-with-me . Specifically, parameters that control the repetitions of the arm movements are defined in the interactionManager.ini as nrep-show and nrep-perform , respectively for the observation phase (set to 8 by default) and the imitation phase, both direct and occluded (set to 16 by default). The command start_occlusion can be sent after the eighth repetition of the movement within the direct phase, to have 8 repetitions for each phase.","title":"Run the virtual demo"}]}